omprises an instruction dispatcher that dispatches instructions executable by the processor and that selectively designates data as cacheable by only one of the L0 caches. The designation of data as cacheable by only one of the L0 caches preferably occurs at the time instructions are dispatched by the instruction dispatcher (i.e., at dispatch time). For example, an instruction dispatch circuit may be provided that designates data as cacheable by only one of the L0 caches based on a portion of a linear address for the data.</p><p>A significant advantage of the inventive processor system is that each L0 cache is associated with (e.g., is \u201ctightly coupled\u201d to) only one execution unit so that L0 cache design is greatly simplified. For example, because each L0 cache is accessed by only one execution unit, arbitration for L0 cache access is not required (e.g., cache arbitration circuitry within each L0 cache is unnecessary), and cache access occurs at the fastest possible speeds (e.g., is not limited by arbitration delays). Further, because memory locations are not shared between L0 caches, L0 cache resources are maximized (e.g., all L0 cached data is non-duplicative data). The addresses assigned to the L0 caches may be assigned without regard for the current thread or task so that assigning and managing task algorithms are not required; and the small size of the L0 caches allows the L0 caches to be located near its associated execution unit (e.g., reducing wiring lengths and thus signal propagation delays).</p><p>Other objects, features and advantages of the present invention will become more fully apparent from the following detailed description of the preferred embodiments, the appended claims and the accompanying drawings.</p><?BRFSUM description=\"Brief Summary\" end=\"tail\"?><?brief-description-of-drawings description=\"Brief Description of Drawings\" end=\"lead\"?><h4>BRIEF DESCRIPTION OF THE DRAWINGS</h4><p>The present invention is described with reference to the accompanying drawings. In the drawings, like reference numbers indicate identical or functionally similar elements. Additionally, the left-most digit of a reference number identifies the drawing in which the reference number first appears.</p><p>FIG. 1 is a block diagram of an inventive processor system configured in accordance with the present invention;</p><p>FIG. 2 is a pipeline timing diagram for a single-cycle load instruction within the inventive processor system of FIG. 1; and</p><p>FIG. 3 is a schematic diagram of an instruction dispatch circuit for dispatching load/store instructions within the inventive processor system of FIG. <b>1</b>.</p><?brief-description-of-drawings description=\"Brief Description of Drawings\" end=\"tail\"?><?DETDESC description=\"Detailed Description\" end=\"lead\"?><h4>DETAILED DESCRIPTION OF THE PREFERRED EMBODIMENTS</h4><p>FIG. 1 is a block diagram of an inventive processor system <b>101</b> configured in accordance with the present invention. The inventive processor system <b>101</b> comprises a processor <b>103</b> coupled to a system bus <b>105</b> comprising a 64-bit data bus <b>107</b><i>a </i>and a 32-bit address bus <b>107</b><i>b. </i>The system bus <b>105</b> couples the processor <b>103</b> to a variety of other components such as a memory controller, an L2 cache, input/output devices and the like (not shown), and allows the processor <b>103</b> to read information from and write information to these components.</p><p>The processor <b>103</b> comprises a bus interface unit <b>109</b> coupled to the system bus <b>105</b>, an L1 cache <b>111</b> coupled to the bus interface unit <b>109</b> and a first level-0 (L0) cache <b>113</b>, a second L0 cache <b>115</b> and an instruction translation look-aside buffer (TLB) <b>117</b> coupled to the L1 cache <b>111</b>. The L1 cache <b>111</b> stores both instructions and data and is accessed via \u201cphysical\u201d addresses (described below). Access to the L1 cache <b>111</b> is arbitrated by an arbiter <b>111</b><i>a </i>internal to the L1 cache <b>111</b>.</p><p>The processor <b>103</b> further comprises an instruction fetch unit <b>119</b> coupled to the L1 cache <b>111</b> and to the instruction TLB <b>117</b>, an instruction decoder <b>121</b> coupled to the instruction fetch unit <b>119</b>, an address generator <b>123</b> coupled to the instruction decoder <b>121</b> and an instruction dispatcher <b>125</b> coupled to the address generator <b>123</b>. A floating point unit <b>127</b>, an integer execution unit <b>129</b>, a first load/store unit <b>131</b> and a second load/store unit <b>133</b> also are provided, and each is coupled to the instruction dispatcher <b>125</b>. The first load/store unit <b>131</b> is coupled to the first L0 cache <b>113</b> and to a data TLB <b>135</b>, and the second load/store unit <b>133</b> is coupled to the second L0 cache <b>115</b> and to the data TLB <b>135</b> such that the first L0 cache <b>113</b> is accessible only by the first load/store unit <b>131</b> and the second L0 cache <b>115</b> is accessible only by the second load/store unit <b>133</b>.</p><p>The processor <b>103</b> further comprises floating point registers <b>137</b> coupled to the floating point unit <b>127</b>, to the first load/store unit <b>131</b> and to the second load/store unit <b>133</b>, and general purpose registers <b>139</b> coupled to the address generator <b>123</b>, to the integer execution unit <b>129</b>, to the first load/store unit <b>131</b> and to the second load/store unit <b>133</b>.</p><p>In operation, the instruction fetch unit <b>119</b> requests an instruction from the L1 cache <b>111</b> by sending a virtual or \u201clinear\u201d address (e.g., an address into the total possible memory space of the inventive processor system <b>101</b>) to the instruction TLB <b>117</b>. In the inventive processor system <b>101</b> of FIG. 1, the virtual address is 32 bits for a total possible memory space of about four gibabytes. The instruction TLB <b>117</b> translates the virtual address into a physical address (e.g., an address into the physically available memory space of the inventive processor system <b>101</b>) and sends the physical address and a fetch request to the L1 cache <b>111</b>. The arbiter <b>111</b>a arbitrates the fetch request with any requests from the bus interface unit <b>109</b>, the first L0 cache <b>113</b> and the second L0 cache <b>115</b>.</p><p>Assuming the requested instruction resides within the L1 cache <b>111</b>, the L1 cache <b>111</b> sends the requested instruction to the instruction fetch unit <b>119</b>, and the instruction fetch unit <b>119</b> passes the instruction to instruction decoder <b>121</b>. In response thereto, the instruction decoder <b>121</b> identifies the type of instruction, the locations of operands required for the instruction (e.g., in memory, in a register, etc.) and the location to which to return results of executing the instruction. In the embodiment of FIG. 1, the instruction set of the processor <b>103</b> defines instructions for accessing memory (e.g., load/store instructions) that are unique from instructions for operating on data (e.g., non-memory instructions such as add, jump, etc.). It will be understood that a more complex instruction set that operates directly on memory operands may be employed with the processor <b>103</b> (e.g., by first decomposing complex instructions into load/store instructions and non-memory instructions).</p><p>If the instruction identified by the instruction decoder <b>121</b> is a load/store instruction, the instruction decoder <b>121</b> forwards the instruction to the address generator <b>123</b>; otherwise, the instruction decoder <b>121</b> bypasses the address generator <b>123</b> and forwards the instruction to the instruction dispatcher <b>125</b>. The address generator <b>123</b> calculates a virtual data address for any data operand associated with a load/store instruction from immediate (e.g., hard-coded) or displacement (e.g., offset) fields within the encoded instruction and/or from values held in the general purpose registers <b>139</b></p><p>The address generator <b>123</b> forwards the load/store instruction and its associated virtual data address to the instruction dispatcher <b>125</b>. Thereafter, the instruction dispatcher <b>12</b>S assigns the instruction either to the first load/store unit <b>131</b> or to the second load/store unit <b>133</b> and forwards the instruction thereto. Specifically, the instruction dispatcher <b>125</b> uses a bit from the virtual data address accompanying the instruction to determine which of the load/store units <b>131</b>, <b>133</b> is to receive the instruction. Preferably, the bit employed to select one of the load/store units <b>131</b>, <b>133</b> is programmably selectable as described below with reference to FIG. <b>3</b>.</p><p>Once the instruction is forwarded to one of the load/store units <b>131</b>, <b>133</b>, the load/store unit receiving the instruction performs the load/store operation designated by the instruction. For example, if the instruction dispatcher <b>125</b> transfers a load instruction to the second load/store unit <b>133</b>, the second load/store unit <b>133</b> executes the load instruction by first determining whether the second L0 cache <b>115</b> contains the data required for the load instruction. More specifically, the second load/store unit <b>133</b> sends the virtual data address of the data to be loaded to the data TLB <b>135</b>, and the data TLB <b>135</b> translates the \u201cpage\u201d portion of the virtual address into a \u201cphysical\u201d page. The physical address then is forwarded to the second L0 cache <b>115</b> for cache row decoding and for use in indexing the bytes of one of the 32-byte lines via offset bits of the physical address. The tag bit portion of the physical address is used to perform a tag compare operation on the array cells of the identified cache row and, if a tag of one of the array cells matches the tag bits, the offset bits are used to identify the appropriate data byte from the 32-byte line within the array cell. Thereafter, the data byte is transferred from the second L0 cache <b>115</b> to the second load/store unit <b>133</b>. The data byte then may be forwarded directly to the floating point registers <b>137</b> or to the general purpose registers <b>139</b>.</p><p>If the tag compare operation fails and the second L0 cache <b>115</b> does not contain the data to be loaded (i.e., a \u201cmiss\u201d), the second L0 cache <b>115</b> sends a request for the data to the L1 cache <b>111</b>. If the L1 cache ill contains the data, the data is transferred from the L1 cache <b>111</b> to the second L0 cache <b>115</b> and from the second L0 cache <b>115</b> to the target location. However, if the L1 cache <b>111</b> does not contain the data, the L1 cache <b>111</b> sends a request for the data to the next memory level (e.g., an L2 cache, system memory, etc.). This process is repeated until the data is found.</p><p>If the instruction dispatcher <b>125</b> transfers a store instruction to one of the load/store units <b>131</b>, <b>133</b>, a similar operation is performed. For example, if the instruction dispatcher <b>125</b> transfers a store instruction to the second load/store unit <b>133</b>, the second load/store unit <b>133</b> executes the store instruction. The contents of the second L0 cache <b>115</b> are examined to ensure that the appropriate cache row is present within the second L0 cache <b>115</b>, and if not, the cache row is retrieved from another memory location (e.g., the L1 cache <b>111</b>, an L2 cache, system memory, etc.). Thereafter, data is transferred from either the floating point registers <b>137</b> or the general purpose registers <b>139</b> to the second load/store unit <b>133</b>, and from the second load/store unit <b>133</b> to the second L0 cache <b>115</b>.</p><p>If the instruction identified by the instruction decoder <b>121</b> is a non-memory instruction, the instruction dispatcher <b>125</b> assigns the instruction to either the floating point unit <b>127</b> or the integer execution unit <b>129</b>, depending on the instruction type. If more than one floating point unit or integer unit is present (not shown), the instruction dispatcher <b>125</b> may employ more sophisticated algorithms for assigning non-memory instructions to a particular floating point or integer unit, as are known in the art.</p><p>Instructions assigned to the floating point unit <b>127</b> read operands from the floating point registers <b>137</b>, perform data operations on the operands and write operation results back to the floating point registers <b>137</b>. L1 kewise, instructions assigned to the integer execution unit <b>129</b> read operands from the general purpose registers <b>139</b>, perform data operations on the operands and write operation results back to the general purpose registers <b>139</b>. The first load/store unit <b>131</b> and the second load/store unit <b>133</b> both have access to the floating point registers <b>137</b> and to the general purpose registers <b>139</b> to allow data transfer between the floating point registers <b>137</b>, the general purpose registers <b>139</b> and the first L0 cache <b>113</b>, and between the floating point registers <b>137</b>, the general purpose registers <b>139</b> and the second L0 cache <b>115</b>.</p><p>In the preferred embodiment, the L1 cache <b>111</b> is a 64K, four-way set associative cache with 32-byte lines, and each L0 cache <b>113</b>, <b>115</b> is a 4K, two-way set associative cache with 32-byte lines. Other cache types (e.g., different sizes, different ways, etc.) may be employed. However, the preferred cache types allow the data TLB <b>135</b> and the first L0 cache <b>113</b> or the second L0 cache <b>115</b> to be accessed within the same CPU cycle (e.g., because no tag address bits are required to identify the desired cache row and the desired byte within each 32-byte line). The 64K cache organization for the L1 cache <b>111</b> (e.g., the size thereof) requires the use of two physical address bits to identify the desired cache row of the L1 cache <b>111</b> so that address translation by the instruction TLB <b>117</b> must occur before L1 cache access. L1 cache access thereby requires two CPU cycles. However, because the physical address must be ready prior to L1 cache access by the instruction TLB <b>117</b>, fewer 32-byte lines per cache row are required (e.g., longer decode delays may be tolerated) and the L1 tag compare is greatly simplified.</p><p>A significant advantage of the inventive processor system <b>101</b> is that the first L0 cache <b>113</b> and the second L0 cache <b>115</b> are \u201ctightly coupled\u201d to the first load/store unit <b>131</b> and to the second load/store unit <b>133</b>, respectively. That is, because the instruction dispatcher <b>125</b> dispatches instructions to the first load/store unit <b>131</b> and to the second load/store unit <b>133</b> based on address bits, no memory address contents are simultaneously held in more than one of the first L0 cache <b>113</b> and the second L0 cache <b>115</b>. Accordingly, the first L0 cache <b>113</b> is accessed only by the first load/store unit <b>131</b>, the second L0 cache <b>115</b> is accessed only by the second load/store unit <b>133</b> and L0 cache design is greatly simplified. For example, because each L0 cache <b>113</b>, <b>115</b> is accessed by only one load/store unit, arbitration for cache access is not required (e.g., rendering cache arbitration circuitry within each L0 cache unnecessary), and cache access occurs at the fastest possible speed (e.g., cache access is not limited by arbitration delays). Coherency between the L0 caches <b>113</b>, <b>115</b> is maintained by virtue of the dispatcher <b>125</b> and without the use of complex coherency circuitry.</p><p>Another advantage of the inventive processor system <b>101</b> is that the tightly coupled nature of the L0 caches <b>113</b>, <b>115</b> yields the largest \u201clogical size\u201d for the L0 cache \u201cpool\u201d (e.g., the amount of non-duplicative cache memory). Assuming each L0 cache <b>113</b>, <b>115</b> is a 4K cache (as preferred), because no memory address contents are simultaneously held in more than one of the first L0 cache <b>113</b> and the second L0 cache <b>115</b> (requiring coherency control), the logical size of the L0 cache pool is always 8K. If memory address contents could be simultaneously held in more than one of the first L0 cache <b>113</b> and the second L0 cache <b>115</b>, the logical size of the L0 cache pool would vary between 4K and 8K, making inefficient use of L0 cache resources. Thus, the inventive processor system <b>101</b> allows maximum utilization of the L0 cache resources. Note that addresses are assigned to the first L0 cache <b>113</b> and to the second L0 cache <b>115</b> without regard for the current thread or task that is running so that algorithms for assigning and managing tasks are not required.</p><p>The use of small L0 caches allows the first L0 cache <b>113</b> to be located near the first load/store unit <b>131</b>, and the second L0 cache <b>115</b> to be located near the second load/store unit <b>133</b>. Wiring lengths and signal propagation times between components thereby are reduced. Accordingly, with short wiring lengths, small cache sizes and no cache arbitration, single cycle access to the first L0 cache <b>113</b> and to the second L0 cache <b>115</b> is easily achieved.</p><p>Because the first L0 cache <b>113</b> and the second L0 cache <b>115</b> are smaller than the L1 cache <b>111</b>, the L0 cache pool holds less data than the L1 cache <b>111</b> and the miss rate of the L0 cache pool is higher than the miss rate of the L1 cache <b>111</b>. However, due to the fast access times (e.g., higher operating frequency) of the first L0 cache <b>113</b> and the second L0 cache <b>115</b>, the average access time for obtaining memory operands within the inventive processor system <b>101</b> is significantly reduced over the average access time for a conventional processor system employing only a large L1 cache. For instance, assume a conventional processor system has a 128K L1 cache with single-cycle access, a one percent L1 cache miss rate and a maximum single-cycle cache access operating frequency of 250 MHZ. If an L1 cache miss requires four CPU cycles to service (e.g., to obtain the data from another memory location such as from an L2 cache, system memory, etc.), the conventional processor system has an average memory access of 1.03 CPU cycles.</p><p>Assume further the inventive processor system <b>101</b> is employed and the L1 cache <b>111</b> is a 64K cache with two-cycle access and a three percent miss rate, and the first L0 cache <b>113</b> and the second L0 cache <b>115</b> are 4K caches each with single-cycle access and a ten percent miss rate. If an L1 cache miss requires four CPU cycles to service, the inventive processor system <b>101</b> has an average memory access of 1.16 CPU cycles. However, because the access time of the L1 cache <b>111</b> has been relaxed to two cycles and because of the design of the first L0 cache <b>113</b> and the second L0 cache <b>115</b> (as previously described), the maximum single-cycle cache access operating frequency of the inventive processor system <b>101</b> may be raised to 300 MHZ. The average memory access time for the inventive processor system <b>101</b> thereby is six percent faster than the conventional processor system's average memory access time due to the higher operating frequency of the inventive processor system <b>101</b> (despite the inventive processor system <b>101</b>'s higher miss rate).</p><p>FIG. 2 is a pipeline timing diagram <b>201</b> for a single-cycle load instruction within the inventive processor system <b>101</b>. With reference to FIG. 2, during CPU cycle 1, the instruction fetch unit <b>119</b> fetches an instruction from the L1 cache <b>111</b> (e.g., via the instruction TLB <b>117</b> as previously described) and passes the instruction to the instruction decoder <b>121</b>. Thereafter, during CPU cycle 2, the instruction decoder <b>121</b> identifies the instruction as a load instruction, identifies the location of data required for the load instruction (e.g., the memory address containing the data to be loaded) and identifies the location to which to return results of executing the load instruction (e.g., a register within the floating point registers <b>137</b> or within the general purpose registers <b>139</b>). Because the instruction is a load instruction, the instruction decoder <b>121</b> forwards the instruction to the address generator <b>123</b>.</p><p>During the first half of the CPU cycle 3, the address generator <b>123</b> calculates a virtual data address for the data associated with the load instruction (as described) and forwards the load instruction and the virtual data address to the instruction dispatcher <b>125</b>. In response thereto, during the second half of the CPU cycle 3, the instruction dispatcher <b>125</b> assigns the load instruction either to the first load/store unit <b>131</b> or to the second load/store unit <b>133</b>. The simplicity of the dispatch algorithm (described further below with reference to FIG. 3) allows the virtual data address calculation and instruction dispatch to occur in one CPU cycle.</p><p>Thereafter, during the first half of the CPU cycle 4, the load/store unit to which the load instruction is dispatched receives the load instruction from the instruction dispatcher <b>125</b> and begins execution of the load instruction. For example, if the second load/store unit <b>133</b> receives the load instruction, the second load/store unit <b>133</b> sends the virtual data address of the data to be loaded to the data TLB <b>135</b>, the data TLB <b>135</b> translates the page portion of the virtual address into a physical page (i.e., the TLB lookup) and a cache row and its associated tags are identified via the remainder of the physical address (i.e., the L0 tag lookup). During the second half of the CPU cycle 4, the tag compare operation (i.e., the L0 tag compare) is performed between the tags associated with the identified cache row and the physical page to identify if the desired data is within the second L0 cache <b>115</b>, hit or miss information is returned, and, if the data is present within the second L0 cache <b>115</b>, the data is returned. Because of the small size and lack of arbitration required to access the L0 caches <b>113</b>, <b>115</b>, the data TLB lookup, the L0 tag lookup, the L0 tag compare and the hit/miss and data return may be performed within one CPU cycle. In CPU cycle 5, the load instruction is completed by writing the identified data to either the floating point registers <b>137</b> or to the general purpose registers <b>139</b>.</p><p>FIG. 3 is a schematic diagram of an instruction dispatch circuit <b>301</b> for dispatching load/store instructions from the instruction dispatcher <b>125</b> to the first load/store unit <b>131</b> and to the second load/store unit <b>133</b>. The instruction dispatch circuit <b>301</b> comprises a linear address result register <b>303</b> coupled to the first load/store unit <b>131</b> and to the second load/store unit <b>133</b> (not shown), and a mask &amp; merge circuit <b>305</b> having a data input coupled to the linear address result register <b>303</b> and a data output coupled to the first load/store unit <b>131</b> and to the second load/store unit <b>133</b>. The instruction dispatch circuit <b>301</b> further comprises an AND gate <b>307</b> having a first input coupled to the linear address result register <b>303</b>, a zero detect circuit <b>309</b> having an input coupled to an output of the AND gate <b>307</b>, and a programmable dispatch select register <b>311</b> coupled to a second input of the AND gate <b>307</b>. A negate circuit <b>313</b> is coupled to the programmable dispatch select register <b>311</b>, and a negate register <b>315</b> is coupled between the mask &amp; merge circuit <b>305</b> and the negate circuit <b>313</b>.</p><p>In operation, the linear address result register <b>303</b> stores each virtual or \u201clinear\u201d data address generated by the address generator <b>123</b>. In the instruction dispatch circuit <b>301</b> of FIG. 3, the virtual data address comprises a 5-bit offset field <b>317</b> for accessing the bytes within a particular 32-byte line of a cache row, a 7-bit cache row field <b>319</b> for selecting one of <b>128</b> cache rows and a 20-bit virtual tag <b>321</b> (e.g., a virtual page number) which is translated by the data TLB <b>135</b> into a \u201cphysical\u201d tag (e.g., a physical page number). The physical tag is compared to the L0 cache's tags during a tag compare operation.</p><p>For the inventive processor system <b>101</b> of FIG. 1, the instruction dispatch circuit <b>301</b> employs one of the bits within the 7-bit cache row field <b>319</b> to select which of the first load/store unit <b>131</b> and the second load/store unit <b>133</b> is to receive an instruction. However, for proper instruction dispatch, if four L0 caches are employed, 2 bits of the 7-bit cache row field <b>319</b> are required and if eight L0 caches are employed, 3 bits of the 7-bit cache row field <b>319</b> are required.</p><p>The particular bit of the 7-bit cache row field <b>319</b> that selects which L0 load/store unit <b>131</b>, <b>133</b> receives an instruction is set by the programmable dispatch select register <b>311</b>. To designate a particular bit of the 7-bit cache row field <b>319</b> as a \u201cselect bit\u201d, the desired bit is set to a logical one within the programmable dispatch select register <b>311</b> and all other register bits are set to a logical zero. Thus, the programmable dispatch select register <b>311</b> may be used to \u201ctune\u201d instruction dispatching by the instruction dispatch circuit <b>301</b> as necessary for improved cache performance.</p><p>The contents of the programmable dispatch select register <b>311</b> are ANDed with the 7-bit cache row field <b>319</b> via the AND gate <b>307</b> (e.g., each bit within the cache row field <b>319</b> is ANDed with its corresponding bit within the programmable dispatch select register <b>301</b> to produce seven AND results), and the results of the AND operations are input to the zero detect circuit <b>309</b>. If the zero detect circuit <b>309</b> detects all zeros, the instruction dispatcher <b>125</b> transfers the instruction to the first load/store unit <b>131</b>, and if the zero detect circuit <b>309</b> detects a one, the instruction dispatcher <b>125</b> transfers the instruction to the second load/store unit <b>133</b> (or vice-versa). In this manner, the two L0 caches <b>113</b>, <b>115</b> never contain the same data, and the largest possible L0 cache pool is maintained.</p><p>As stated, the first and the second L0 caches <b>113</b>, <b>115</b> preferably are 4K, two-way set associative caches with 32-byte lines per cache row. Thus, each cache requires 12 bits to access the data within the cache (e.g., 2<sup>12</sup>=4096). Five offset bits are required to access the bytes within each 32-byte line of a cache row and six bits are required to access one of the 64 cache rows within the cache. Thus, because the L0 caches <b>113</b>, <b>115</b> are 4K, two-way set associative caches with 32 byte-sets, only six of the seven cache row bits of the 7-bit cache row field <b>319</b> are employed to access each cache row. The 7<sup>th </sup>bit in this example is used to select one of the L0 caches <b>113</b>, <b>115</b> during instruction dispatch.</p><p>The six bits used for cache row access must be separated from the original seven bits of the 7-bit cache row field <b>319</b> because the select bit is a programmed bit. To separate the six bits, the contents of th