he misses (which is again particularly troublesome with misses of demand requests, rather than speculative requests). Finally, in multi-processor systems wherein one or more caches are shared by a plurality of processors, prefetching can result in uneven (and inefficient) use of the cache with respect to the sharing processors.</p><p>Another cause of delay related to multi-level cache hierarchies is the need to access a directory for each level, typically contained within that particular storage level. Directories provide means for indexing values in the data portion of the cache, and also maintain information about whether a cache entry is valid or whether it is \u201cdirty\u201d which means that the data is conditionally invalid due to access by another cache user in a multiprocessor system. Entries in a directory are matched with addresses of values to determine whether the value is present in the level, or must be loaded. The presence of a value is determined by comparing the tag associated with the address of that value with entries in the directory. This is a time consuming process, which can stall the access to the cache waiting for the match to be found.</p><p>In light of the foregoing, it would be desirable to provide a method of speeding up core processing by improving the prefetching and cache mechanisms, particularly with respect to the interaction of the prefetching mechanism with the cache hierarchy. It would be further advantageous if the method allowed a programmer to optimize various features of the prefetching mechanism.</p><h4>SUMMARY OF THE INVENTION</h4><p>It is therefore one object of the present invention to provide an improved processor for a computer system, having a prefetch mechanism for instructions and/or operand data.</p><p>It is another object of the present invention to provide an improved data processing system using such a processor, which also has one or more caches in the memory hierarchy.</p><p>It is yet another object of the present invention to provide a computer system which makes more efficient use of a cache hierarchy working in conjunction with prefetching.</p><p>The foregoing objects are achieved in a method and apparatus for operating a multi-level memory hierarchy of a computer system, comprising the steps of determining that a first load request has been cancelled, evaluating that a second load request is speculatively loaded dependent on the first load request, and setting a flag in a tag entry to indicate that the second load request is invalid. The request can be for operand data or an instruction. The first load request may be cancelled by the receipt of a cancel indication, received by a cache management controller and the cancel indication may be issued after a finite number of cycles have occurred subsequent to the request. The flag in the tag entry may be more than one flag, the use of two complementary bits making certain logic comparisons simpler.</p><p>The above as well as additional objectives, features, and advantages of the present invention will become apparent in the following detailed written description.</p><?BRFSUM description=\"Brief Summary\" end=\"tail\"?><?brief-description-of-drawings description=\"Brief Description of Drawings\" end=\"lead\"?><h4>BRIEF DESCRIPTION OF THE DRAWINGS</h4><p>The novel features believed characteristic of the invention are set forth in the appended claims. The invention itself, however, as well as a preferred mode of use, further objectives, and advantages thereof, will best be understood by reference to the following detailed description of an illustrative embodiment when read in conjunction with the accompanying drawings, wherein:</p><p>FIG. 1 is a block diagram of a conventional superscalar computer processor, depicting execution units, buffers, registers, and the on-board (L<b>1</b>) data and instruction caches;</p><p>FIG. 2 is an illustration of one embodiment of a data processing system in which the present invention can be practiced;</p><p>FIG. 3 is a block diagram illustrating selected components that can be included in the data processing system of FIG. 2 according to the teachings of the present invention;</p><p>FIG. 4 is a block diagram showing connection of a CPU, L<b>2</b> cache, bus and memory constructed in accordance with the present invention;</p><p>FIG. 5 is a flow diagram showing one embodiment of a decision tree of a method for accessing a memory hierarchy;</p><p>FIG. 6 is a flow diagram of a decision tree for determining actions to take on receipt of a cancel indication in accordance with an embodiment of the present invention; and</p><p>FIG. 7 is a block diagram of a cache memory hierarchy constructed in accordance with one embodiment of the present invention.</p><?brief-description-of-drawings description=\"Brief Description of Drawings\" end=\"tail\"?><?DETDESC description=\"Detailed Description\" end=\"lead\"?><h4>DESCRIPTION OF AN ILLUSTRATIVE EMBODIMENT</h4><p>With reference now to the figures, and in particular with reference to FIG. 2, a data processing system <b>120</b> is shown in which the present invention can be practiced. The data processing system <b>120</b> includes processor <b>122</b>, keyboard <b>182</b>, and display <b>196</b>. Keyboard <b>182</b> is coupled to processor <b>122</b> by a cable <b>128</b>. Display <b>196</b> includes display screen <b>130</b>, which may be implemented using a cathode ray tube (CRT), a liquid crystal display (LCD), an electrode luminescent panel or the like. The data processing system <b>120</b> also includes pointing device <b>184</b>, which may be implemented using a track ball, a joy stick, touch sensitive tablet or screen, track path, or as illustrated a mouse. The pointing device <b>184</b> may be used to move a pointer or cursor on display screen <b>130</b>. Processor <b>122</b> may also be coupled to one or more peripheral devices such a modem <b>192</b>, CD-ROM <b>178</b>, network adapter <b>190</b>, and floppy disk drive <b>140</b>, each of which may be internal or external to the enclosure or processor <b>122</b>. An output device such as a printer <b>100</b> may also be coupled with processor <b>122</b>.</p><p>It should be noted and recognized by those persons of ordinary skill in the art that display <b>196</b>, keyboard <b>182</b>, and pointing device <b>184</b> may each be implemented using any one of several known off-the-shelf components.</p><p>Reference now being made to FIG. 3, a high level block diagram is shown illustrating selected components that can be included in the data processing system <b>120</b> of FIG. 2 according to the teachings of the present invention. The data processing system <b>120</b> is controlled primarily by computer readable instructions, which can be in the form of software, wherever, or by whatever means such software is stored or accessed. Such software may be executed within the Central Processing Unit (CPU) <b>150</b> to cause data processing system <b>120</b> to do work.</p><p>Memory devices coupled to system bus <b>105</b> include Random Access Memory (RAM) <b>156</b>, Read Only Memory (ROM) <b>158</b>, and nonvolatile memory <b>160</b>. Such memories include circuitry that allows information to be stored and retrieved. ROMs contain stored data that cannot be modified. Data stored in RAM can be changed by CPU <b>150</b> or other hardware devices. Nonvolatile memory is memory that does not lose data when power is removed from it. Nonvolatile memories include ROM, EPROM, flash memory, or battery-pack CMOS RAM. As shown in FIG. 3, such battery-pack CMOS RAM may be used to store configuration information.</p><p>An expansion card or board is a circuit board that includes chips and other electronic components connected that adds functions or resources to the computer. Typically, expansion cards add memory, disk-drive controllers <b>166</b>, video support, parallel and serial ports, and internal modems. For lap top, palm top, and other portable computers, expansion cards usually take the form of PC cards, which are credit card-sized devices designed to plug into a slot in the side or back of a computer. An example of such a slot is PCMCIA slot (Personal Computer Memory Card International Association) which defines type I, II and III card slots. Thus, empty slots <b>168</b> may be used to receive various types of expansion cards or PCMCIA cards.</p><p>Disk controller <b>166</b> and diskette controller <b>170</b> both include special purpose integrated circuits and associated circuitry that direct and control reading from and writing to hard disk drive <b>172</b>, and a floppy disk or diskette <b>174</b>, respectively. Such disk controllers handle tasks such as positioning read/write head, mediating between the drive and the CPU <b>150</b>, and controlling the transfer of information to and from memory. A single disk controller may be able to control more than one disk drive.</p><p>CD-ROM controller <b>176</b> may be included in data processing <b>120</b> for reading data from CD-ROM <b>178</b> (compact disk read only memory). Such CD-ROMs use laser optics rather than magnetic means for reading data.</p><p>Keyboard mouse controller <b>180</b> is provided in data processing system <b>120</b> for interfacing with keyboard <b>182</b> and pointing device <b>184</b>. Such pointing devices are typically used to control an on-screen element, such as a graphical pointer or cursor, which may take the form of an arrow having a hot spot that specifies the location of the is pointer when the user presses a mouse button. Other pointing devices include a graphics tablet, stylus, light pin, joystick, puck, track ball, track pad, and the pointing device sold under the trademark \u201cTrack Point\u201d by International Business Machines Corp. (IBM).</p><p>Communication between processing system <b>120</b> and other data processing systems may be facilitated by serial controller <b>188</b> and network adapter <b>190</b>, both of which are coupled to system bus <b>105</b>. Serial controller <b>188</b> is used to transmit information between computers, or between a computer and peripheral devices, one bit at a time over a single line. Serial communications can be synchronous (controlled by some standard such as a clock) or asynchronous (managed by the exchange of control signals that govern the flow of information). Examples of serial communication standards include RS-232 interface and the RS-422 interface. As illustrated, such a serial interface may be used to communicate with modem <b>192</b>. A modem is a communication device that enables a computer to transmit information over standard telephone lines. Modems convert digital computer signals to interlock signals suitable for communications over telephone lines. Modem <b>192</b> can be utilized to connect data processing system <b>120</b> to an on-line information service or an Internet service provider. Such service providers may offer software that can be down loaded into data processing system <b>120</b> via modem <b>192</b>. Modem <b>192</b> may provide a connection to other sources of software, such as a server, an electronic bulletin board (BBS), or the Internet (including the World Wide Web).</p><p>Network adapter <b>190</b> may be used to connect data processing system <b>120</b> to a local area network <b>194</b>. Network <b>194</b> may provide computer users with means of communicating and transferring software and information electronically. Additionally, network <b>194</b> may provide distributed processing, which involves several computers in the sharing of workloads or cooperative efforts in performing a task. Network <b>194</b> can also provide a connection to other systems like those mentioned above (a BBS, the Internet, etc.).</p><p>Display <b>196</b>, which is controlled by display controller <b>198</b>, is used to display visual output generated by data processing system <b>120</b>. Such visual output may include text, graphics, animated graphics, and video. Display <b>196</b> may be implemented with CRT-based video display, an LCD-based flat panel display, or a gas plasma-based flat-panel display. Display controller <b>198</b> includes electronic components required to generate a video signal that is sent to display <b>196</b>.</p><p>Printer <b>100</b> may be coupled to data processing system <b>120</b> via parallel controller <b>102</b>. Printer <b>100</b> is used to put text or a computer-generated image (or combinations thereof) on paper or on another medium, such as a transparency sheet. Other types of printers may include an image setter, a plotter, or a film recorder.</p><p>Parallel controller <b>102</b> is used to send multiple data and control bits simultaneously over wires connected between system bus <b>105</b> and another parallel communication device, such as a printer <b>100</b>.</p><p>CPU <b>150</b> fetches, decodes, and executes instructions, and transfers information to and from other resources via the computers main data-transfer path, system bus <b>105</b>. Such a bus connects the components in a data processing system <b>120</b> and defines the medium for data exchange. System bus <b>105</b> connects together and allows for the exchange of data between memory units <b>156</b>, <b>158</b>, and <b>160</b>, CPU <b>150</b>, and other devices as shown in FIG. <b>3</b>. Those skilled in the art will appreciate that a data processing system constructed in accordance with the present invention may have multiple components selected from the foregoing, including even multiple processors.</p><p>Referring now to FIG. 4, one embodiment of the present invention allows data processing system <b>120</b> to more efficiently process information, by utilizing hints in the instruction set architecture used by the processor, core of CPU <b>270</b> to exploit prefetching. The CPU <b>270</b> uses, several conventional elements, including a plurality of registers, such as general purpose and special purpose registers (not shown), and a plurality of execution units. CPU <b>270</b> is further comprised of several novel elements such as an instruction fetch unit (IFU) <b>250</b> containing L<b>1</b> instruction cache (I-Cache) <b>252</b>, a load/store unit (LSU) <b>254</b> containing L<b>1</b> operand data cache (D-Cache) <b>256</b>, and a prefetch unit (PFU) <b>258</b>. IFU <b>250</b> and LSU <b>254</b> perform functions which include those performed by conventional execution units, but are further modified to enable the features described hereinafter. IFU <b>250</b> executes instruction fetches, while LSU <b>254</b> executes instructions which either load operand data from memory, or which store data to memory.</p><p>IFU <b>250</b> and LSU <b>254</b> are connected to the on-board (L<b>1</b>) cache. As shown in FIG. 4, the L<b>1</b> cache may actually comprise separate operand data and instruction caches. L<b>1</b> D-cache <b>256</b> and L<b>1</b> I-Cache <b>252</b> are further connected to the lower level storage subsystem which, in the illustrated embodiment, includes at least one additional cache level, L<b>2</b> cache <b>272</b>, which may also be incorporated on-board. L<b>2</b> cache <b>272</b> may in turn be connected to another cache level, or to the main memory <b>286</b>, via system bus <b>284</b>.</p><p>PFU <b>258</b> is linked to CIU (Core Instruction Unit) <b>260</b>. The instruction set architecture (ISA) for the processor core (e.g., the ISA of a PowerPC\u2122 630 processor) is extended to include explicit prefetch instructions (speculative requests). CIU <b>260</b> is aware of PFU <b>258</b> and issues instructions directly to PFU according to bits in the extended instruction which are set by the software (the computer's operating system or user programs). This approach allows the software to better optimize scheduling of load and store operations (prediction techniques in software may be more accurate than hardware). PFU <b>258</b> may be split into an instruction prefetch unit and an operand data prefetch unit.</p><p>Prefetch unit <b>258</b> issues load requests to L<b>2</b> cache controller <b>272</b>, which are queued in reload queue <b>280</b>. In this figure, four reload queues <b>280</b> are shown, but the quantity should be chosen in terms of throughput and device area and can be any number.</p><p>As execution of CPU <b>270</b> proceeds, cache line load requests which were made by PFU <b>258</b> become resolved. Either a commit occurs, which happens when it becomes determined that a particular instruction cache line will be executed, or operand data within that line will be loaded or stored, or the execution of the processor bypasses the use of that cache line, and therefore the line requested is no longer needed.</p><p>Performance can be improved by the use of active cancel and commit commands. These commands can be sent by CPU <b>270</b>, to indicate that a cache line is no longer needed (cancel) or definitely needed (commit). The command can take the form of one or more software signal lines or as an instruction provided to the L<b>1</b> Caches <b>252</b> and <b>254</b>, or L<b>2</b> Cache <b>272</b>. By sending a cancel command to cancel cache requests for lines which are no longer needed as a processor resolves the branch paths through executing, the reload queues <b>280</b> become available, improving the performance of the system, since the reload queues <b>280</b> are a limited resource. The cancel command may also be sent after a predetermined number of instruction cycles have been executed by the CPU <b>270</b> since the load, this has the effect of clearing stale entries. Cancel or commit commands associated with instruction prefetches may be provided by CIU <b>260</b> or the IFU <b>250</b> to the L<b>1</b> instruction cache <b>252</b>. Cancel or commit commands associated with operand data prefetches may be provided by CIU <b>260</b> or the LSU <b>254</b> to the L<b>1</b> data cache <b>256</b>. Committing the cache lines is accomplished by setting one or more bit states which indicate that a particular cache line is to be speculatively loaded to the opposite state. Thus a committed line will now be treated as if it were demand loaded.</p><p>The acceptance of the cancel command, can be conditioned upon the state of a bus or memory being accessed by the corresponding cancel command. Referring again to FIG. <b>4</b> and referring additionally to FIG. 5, a decision diagram is shown for using the state of the system bus <b>284</b> to determine whether to cancel a load request. The cancel command may be ignored if the bus cycle has proceeded to the point where the address lines have been driven onto the bus, unless the bus has entered a wait state waiting for the response from slow memory, in which case the load may be cancelled by issuing a \u201cretry\u201d response from L<b>2</b> cache controller <b>272</b> itself. Further, if a non-retry response is received from the bus snoopers after the address transaction has commenced, the load is allowed to proceed. This has the effect of allowing efficient use of the bus, since once the bus is committed to retrieving a memory value for which the overhead investment is substantial, the load can be allowed to proceed. Since another load request for the same location which was just cancelled could occur soon after the cancel command is allowed to cancel the load, proceeding with the load if the bus cycle has progressed to the driving point allows for more efficient use of the bus.</p><p>FIG. 5 illustrates the mechanics of this decision process. First, an address transaction is initiated (<b>220</b>) on the system bus <b>284</b>. If a cancel indication has been received at this time (<b>222</b>), the request can be cancelled (<b>232</b>). If the bus has not acknowledged the transaction with a grant response (<b>224</b>), the transaction can be cancelled if a cancel indication is received (<b>222</b>). Once the bus grant indication is received, if a cancel indication is received (<b>226</b>), a retry response will be driven onto the system bus <b>284</b> and the request cancelled (<b>232</b>). If a non-retry response is received from the bus prior to any cancel indication being received, the cache is loaded (<b>234</b>). If a retry response is received in step <b>230</b>, the request will be retried (<b>233</b>) if a cancel indication is not received (<b>231</b>), otherwise the request will be cancelled (<b>232</b>).</p><p>Referring now to FIG. 4, one implementation of the present invention uses an L<b>2</b> cache controller <b>272</b> which provides one or more reload queues <b>280</b> which contain request tags/flags <b>282</b>, for each load request, which relate to prefetching. A given reload queue <b>280</b> includes a tag portion containing at least a first flag which indicates whether the entry was retrieved as the result of a speculation. The tag portion can also contains a series of bit fields that indicate that the entry is valid and establish a speculation hierarchy. Each entry that represents a cache line that was speculatively loaded dependent on a prior cache line contains the same upper bit field pattern. Bits are set in each successive bit field to indicate a further order of speculation. For example, 16 bits could be provided in a tag field controlling the allocation of 4 sets of cache. The lower eight bits are a identifier unique to the sets. The top eight bits contain the \u201cvalid\u201d bit fields, indicating that the cache lines are valid entries. Each bit field is two bits wide, comprising a valid and an invalid flag. The top two bits of the tag field correspond to the first set and correspondingly the set loaded with the lowest order of speculation. The next two lower bits correspond to the next lower order of speculation and so forth. Load requests having a higher order of speculation will have the same bit pattern for all of the bit fields above the bit field corresponding to the their order of speculation and the valid bit set for the order of speculation. The use of two bits presents an advantage in logic, making it simpler to test the valid or invalid state.</p><p>When a speculative load request is for a line no longer needed, due to a branch-prediction failure for that entry, or a cancel command being received, the request tags <b>282</b> that indicate that the entry is valid can be reset to the invalid state. Then, the entries that were requested due to speculative dependence on that entry can be freed. The request queue entries can be scanned via a recursive walk-back algorithm within the queue, wherein queue entries can be continually freed by a process that examines the entries to see if the entry with a lower order of speculation is still valid. Alternatively, combinatorial logic can be used to perform the dependence evaluation and freeing of entries. The entry with a lower order of speculation will have the upper portion of its tag entry in common with entries having dependence on it, all of the entries which correspond to load requests that are speculatively dependent on that lower order entry will have the same bits set as the lower order tag entry. Another technique that may be used in combination is where the cache controlled by L<b>2</b> cache controller <b>272</b> is set associative. A particular set in the cache may be assigned to a branch path and that the walk back for cache lines which are related by dependence may be performed by examining class identifier fields in the cache, as well as the bit field in the tag entry.</p><p>A further improvement is made to the operation of a multi-level cache hierarchy by the decision algorithm depicted in FIG. 6, which can be performed by the system depicted in FIG. 4, with the upper level corresponding to L<b>1</b> caches <b>252</b> and <b>256</b>, and the lower level corresponding to L<b>2</b> cache controller <b>272</b>. The load request is received by L<b>2</b> cache controller <b>272</b> when L<b>1</b> cache <b>252</b> or <b>256</b> is missed. CIU <b>260</b> provides indications to L<b>2</b> cache controller <b>272</b> that a load request is either speculative or demand and is for either an instruction or for operand data. Based on this information, speculative loads for operand data are restricted to the cache controlled by L<b>2</b> cache controller <b>272</b>, keeping L<b>1</b> cache <b>256</b> free from speculative operand data loads. Since the frequency of instruction fetches exceeds the frequency of operand data fetches ordinarily, this provides an improvement in the hit rate of the L<b>1</b> D-cache <b>256</b>.</p><p>An even further improvement is also shown in the decision algorithm depicted in FIG. <b>6</b>. Speculative instruction fetches which miss L<b>1</b> cache <b>252</b> generate load requests and if the cache controlled by L<b>2</b> cache controller <b>272</b> is also missed, no action is performed. This has the effect of keeping speculative instruction loads out of both L<b>1</b> cache <b>252</b> and the L<b>2</b> cache, unless they are for frequently used instructions, and further provides the benefit of reducing system bus bandwidth use.</p><p>The mechanics of the exemplary methods embodied in FIG. 6 are as follows: After a load request is received (<b>350</b>), if the load is not speculative (<b>352</b>) and the lower cache is missed (<b>354</b>), the lower level cache is first loaded (<b>364</b>). Then, the upper level cache is loaded (<b>366</b>), then the LRU is updated (<b>368</b>). For speculative requests, if the request is not for an instruction fetch (<b>356</b>), if the lower cache is missed (<b>358</b>), only the lower level cache is loaded (<b>362</b>). This keeps speculative operand data requests out of the upper level cache. If the lower cache is not missed, the LRU is updated (<b>368</b>). If the speculative request is for an instruction fetch (<b>356</b>) if the lower level cache already contains the prefetch values (<b>360</b>), the upper level cache is loaded (<b>366</b>) and the LRU updated (<b>368</b>), otherwise the request is ignored. This keeps speculative instruction fetches out of the L<b>1</b> and L<b>2</b> caches unless they are for frequently used instructions.</p><p>The operation of the address comparison needed to determine cache hits or misses can be improved, as well as general access to the directories of memory subsystems in a multi-level memory hierarchy. Referring to FIG. 7, in an exemplary embodiment, this corresponds to the directory of L<b>2</b> cache <b>312</b>, but may extend to further levels of cache and storage systems other than semiconductor memory. L<b>1</b> cache is divided into an L<b>1</b> instruction cache <b>304</b> and an L<b>1</b> data cache <b>306</b>. L<b>1</b> instruction cache <b>304</b> is a set associative cache, containing for example eight sets, and at least one of those sets is dedicated to containing the directory for the L<b>2</b> Cache <b>312</b>. This provides much faster access to the directory information and much faster address matching to determine cache hits, as the address comparators <b>314</b> can be directly connected to the directory set <b>312</b> of L<b>1</b> cache <b>304</b>. The presence of the L<b>2</b> Cache directory information in L<b>1</b> Cache <b>304</b> rather than the L<b>2</b> Cache <b>308</b> provides faster access due to the faster access times of the L<b>1</b> Cache <b>304</b>. This technique avoids having to load the directory from the L<b>2</b> cache into the L<b>1</b> cache, or use techniques commonly known in the art as lookaside or read-thro