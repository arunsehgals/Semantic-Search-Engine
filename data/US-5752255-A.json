troughs in the number of systems communicating with one another over the networks at any particular time of the day. The number of network addresses regularly used by a particular processor at any one time is also therefore likely to change. This dynamic nature of the networks will effectively alter the optimum cache size for any one processor 34.</p><p>Referring now to FIG. 3, a cache memory 50 according to the present invention and as stored in synchronous data memory 36 is represented as a series of pages 60-1, 60-2, 60-3 . . . 60-m. Each page is identified in memory by its page address 61. Each page includes addressable memory space subdivided into entries 62-1, 62-2, 62-3 . . . 62-n. Each entry 62 is identified by its entry address 63. In the example shown in the figure, each page 60 includes 2<sup>7</sup> addressable locations 62 (n=128), and the cache 50 includes 2<sup>4</sup> pages 60 (m=16). The relative sizes of pages 60 and cache 50 are exemplary only, and determine the granularity of the cache. A page address 61 may be regarded as a starting position in the cache memory 50, and the entry address 63 regarded as an offset to that position. A page, in the context of this application, is thus defined as any portion of contiguous memory space within the cache memory 36 and may be sufficient for the storage of one or more entries 62.</p><p>Each cache entry 62 stores a routing address data 64 for nodes within the various networks 13,15,17 retrieved from the address resolution engine 22. Each individual entry in the address resolution engine 22 (ie. representing each node within the system) is uniquely identified by, for example, the maximum size OSI 20-byte address. In order to avoid the necessity of performing a 20-byte comparison when searching for address data in the address resolution engine 22, this is compressed to a 32-bit hashed key using techniques well known in the art. Such a key 70 is shown in FIG. 4. The key 70 is used to determine the cache address 61,63 into which the routing address data will be placed. The key is 32 bits long, having a least significant bit 70-1, and a most significant bit 70-32, and is therefore capable of identifying 2<sup>32</sup> (&gt;4\u00d710<sup>9</sup>) addressable locations. The cache 50, on the other hand, has 2<sup>7</sup> multiplied by 2<sup>4</sup> (=2<sup>11</sup> or 2048) addressable locations. A selection process maps the address of the entry in address resolution engine 22 to an address 61,63 in the cache (hereinafter referred to as the cache address) by using seven entry selection bits 72 of the key 70 to identify the entry address 63, and a further four page selection bits 75 to identify the page address 61. Thus bits 70-9 to 70-15 together with 70-18 to 70-21 fully identify the cache address 61,63. The particular bits 70-9 to 70-15 and 70-18 to 70-21 are entirely exemplary, and in reality would be carefully preselected for use by the selection process depending upon the distribution of individual key values 70 throughout the range of possible values arising in the 32-bit hashed address.</p><p>Cache address collisions can be accommodated using standard re-hashing techniques well known in the art. In the present example, a modified quadratic re-hashing scheme is used: if the cache address 61,63 is already occupied by a valid cache entry, then the next sequential address is tried, followed by the third next sequential address. To avoid significant processing overhead, only two re-hash operations are permitted, and if no vacant cache address 61,63 has been found thereafter, the data is not stored in the cache 50. Frequent occurrence of this situation suggests that the cache is too small, or that the selection process has not be optimised for the pattern of likely key values.</p><p>A cache usage monitoring system continually monitors the usage of the cache. This is preferably achieved by determining the ratio of valid cache entries to the number of available cache addresses, but might alternatively utilize factors such as the cache hit rate, the proportion of entries being aged out of the cache, or the number of collisions which must be handled during the cache address selection process. This permits the system to determine whether the present cache size is optimal and making efficient use the valuable high speed memory resource 36. Such cache size monitoring systems are well known in the art, and may form part of a cache refreshment system responsible for the ageing out of old or stale cache entries. A cache refreshment mechanism has been described in co-pending UK Patent Application entitled \"Low overhead, non-coherent cache refreshment mechanism\" filed by Digital Equipment International Limited on the same date as the present application.</p><p>In accordance with the present invention, where the cache usage monitoring system determines that the cache is being under-utilized, the cache is reduced in size by removing a number of pages from the cache. This is effected by a modification to the selection process which maps the 32-bit key address 70 to a cache address 61,63. In the simple example above, this may be achieved by reducing the number of pages from 2<sup>4</sup> to 2<sup>3</sup>, and reducing the number of page selection bits 75 to only include bits 70-18 to 70-20 of key address 70, ie. by removal of most significant bit 70-21 from the page selection bits 75 to form page selection bits 74. It will thus be clear that all entries 62 in the cache 50 resident on pages 60-9 (2<sup>3</sup> +1) to 60-16 (2<sup>4</sup>) are no longer addressable using this new selection process and will effectively be lost. This, however, is transparent to the processor 34: the remaining entries 62 on pages 60-1 to 60-8 will remain accessible using the new selection process, and any requests for data therein will result in a cache hit. A request for data resident in the removed pages will result in the new selection process directing the processor 34 to a different page 60 within the new page address range 60-1 to 60-8, such that a cache miss will be reported in conformity with the normal operation of a non-coherent cache. The processor will then request the data from the ARE 22.</p><p>Conversely, where the cache usage monitoring system determines that the cache is being over-utilized, with consequent detrimental effects on the speed and performance of the system, the cache may be increased in size by adding a number of pages 60 to the cache. This is effected by a modification to the selection process which maps the 32-bit key address 70 to a cache address 61,63 in analogous manner to that described above for reducing the cache size. In this instance, however, the cache size is doubled to 32 (2<sup>5</sup>) pages, and an extra page selection bit 70-22 is 