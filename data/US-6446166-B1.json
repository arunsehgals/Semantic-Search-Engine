he <b>34</b> is first examined to see if the requested memory block is already in the cache. If not (a \u201ccache miss\u201d), the load operation will be entered into a load queue (not shown) of the cache. The load queue severely limits the number of outstanding loads that can be pending in the system. Typically, there are only two or three entries in the load queue, as most systems rely on the assumption that the majority of accesses will be for operand data that is already in the L<b>1</b> cache (cache \u201chits\u201d). If the load queue is already full and another cache miss occurs, the processor core stalls until an entry in the queue becomes available.</p><p>Several other processing delays are associated with the operation of, or interaction with, the caches, particularly the L<b>1</b> cache. For example, on a cache miss with a set associative cache, it is necessary to select a cache line in a particular set of the cache for use with the newly requested data (a process referred to as eviction or victimization). The request cannot be passed down to the lower storage subsystem until a victim is chosen. If the chosen victim has been previously modified (the object of a store operation), then the modified value must be aged out (cast out). The logic unit used to select the victim, such as a least-recently (or less recently) used (LRU) algorithm, must also be updated in the L<b>1</b> cache. These steps are located in the critical path of processor core execution.</p><p>Similarly, a reload buffer (not shown) is used to temporarily hold values before they are written to the L<b>1</b> cache to handle cache read/write collisions. When the lower level memory hierarchy supplies the value requested by a load operation, the response (operand data and address) first enters the reload buffer.</p><p>Delays may likewise occur for store (write) operations. which use a store queue. These types of delays can also arise with operations whose targets are other than register renames, such as instruction fetch units, or translation tables requesting addresses. Translation tables commonly used in processors include translation lookaside buffers which convert physical addresses to virtual addresses (for either instructions or operand data, i.e., ITLBs and DTLBs), or effective-to-real address tables (ERATs).</p><p>An additional delay is presented by the requirement that the entire cache line be received by the L<b>1</b> cache prior to passing the critical value on to the appropriate element within the processor (e.g., to a register rename buffer, translation lookaside buffer, or instruction dispatch unit). In fact, the entire cache line of, say, 64 bytes must be loaded into the L<b>1</b> cache even though the processor only requested an 8-byte word (the L<b>1</b> cache controller provides the smaller granularity on the processor output side).</p><p>As noted above, a cache line victim representing modified data must be written to the lower levels of the memory hierarchy; this is true for a \u201cwrite-back\u201d cache, where data values are not immediately passed on to the remainder of the memory hierarchy after a store operation. Caches can also be \u201cwrite-through,\u201d but this leads to increased demands on bus bandwidth. Write-back caches use state information bits to maintain consistency within the overall memory hierarchy (coherency), combined with the monitoring (snooping) of memory operations. One example of the state information is that supplied by the \u201cMESI\u201d cache coherency protocol, wherein a cache line can be in one of four coherency states: Modified, Exclusive, Shared or Invalid. Cache coherency protocols introduce further complexities and requirements into the interaction of the caches.</p><p>In light of the foregoing, it would be desirable to provide a method of speeding up core processing by improving the operation of the caches, particularly the L<b>1</b> cache. It would be particularly advantageous if the method could provide values (instructions or operand data) more directly to processor components, i.e., without requiring the use of so many intervening queues and buffers, and allow more flexibility in the interaction between a cache and a processor or between vertically adjacent caches (e.g., L<b>1</b> and L<b>2</b>) in a multi-cache hierarchy.</p><h4>SUMMARY OF THE INVENTION</h4><p>It is therefore one object of the present invention to provide an improved data processing system having one or more local caches in the memory hierarchy.</p><p>It is another object of the present invention to provide such an improved data processing system having a multi-level cache structure, and at least one layered cache wherein one or more cache functions are handled by a lower level cache.</p><p>It is yet another object of the present invention to provide a memory structure for a computer system,which speeds up memory accesses by removing or distancing cache functions from the critical path of core execution.</p><p>The foregoing objects are achieved in a method of operating a multi-level cache of a computer system, comprising the steps of monitoring cache activity of an upper level cache and a lower level cache both associated with a processor of the computer system, issuing a request from the processor to load a value (wherein the request misses the upper level cache), and selecting a victim cache block in the upper level cache for receiving the requested value based at least in part on prior cache activity of both cache. The victim selection state of the upper level cache is maintained and updated by the lower level cache controller. This updating is made possible because all load requests issued by the processor are seen by the lower level cache controller regardless of whether the request hits or misses the upper level cache. When the lower level cache controller returns the requested cache line of data due to an upper level cache miss, it also returns the selected victim information which is used to determine where in the upper level cache to write the requested cache lines. This upper level cache victim selection takes place out of a critical path of execution of a core of the processor and may involve, e.g., a least/less recently used algorithm. Thus the method can further include the selection of a victim cache block in the upper level cache for receiving the requested value based at least in part on cache activity of both cache levels.</p><p>The above as well as additional objectives, features, and advantages of the present invention will become apparent in the following detailed written description.</p><?BRFSUM description=\"Brief Summary\" end=\"tail\"?><?brief-description-of-drawings description=\"Brief Description of Drawings\" end=\"lead\"?><h4>BRIEF DESCRIPTION OF THE DRAWINGS</h4><p>The novel features believed characteristic of the invention are set forth in the appended claims. The invention itself, however, as well as a preferred mode of use, further objectives, and advantages thereof, will best be understood by reference to the following detailed description of an illustrative embodiment when read in conjunction with the accompanying drawings, wherein:</p><p>FIG. 1 is a block diagram of a conventional superscalar computer processor, depicting execution units, buffers, registers, and the on-board (L<b>1</b>) data and instruction caches;</p><p>FIG. 2 is an illustration of one embodiment of a data processing. system in which the present invention can be practiced;</p><p>FIG. 3 is a block diagram illustrating selected components that can be included in the data processing system of FIG. 2 according to the teachings of the present invention;</p><p>FIG. 4 is a block diagram of a processing unit constructed in accordance with one embodiment of the present invention, depicting operation of a cache structure which includes an L<b>1</b> operand data cache;</p><p>FIG. 5 is a block diagram of a processing unit constructed in accordance w