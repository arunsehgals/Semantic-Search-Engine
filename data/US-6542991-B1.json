witching. The pulse-based high-speed flip-flop <b>400</b> enables virtually instantaneous switching between threads, saving of the machine state of a stalled thread, and machine state restoration of an activated thread. The fast, nanoseconds range, context switching rapidly controls which thread is activated by the pulse-based high-speed flip-flop <b>400</b>. The thread switch logic <b>610</b> receives a plurality of input signals that evoke a context switch and thread switch. In an illustrative processor, input terminals to the thread switch logic <b>610</b> include an L1_load_miss terminal, an L1_instruction_miss terminal, an instruction_buffer_empty terminal, a thread_priority terminal, an MT_mode terminal, an external_interrupt terminal, and an internal_interrupt terminal. The thread switch logic <b>610</b> generates a thread identifier (TID) signal based on signals to the input terminals. The thread switch logic <b>610</b> generates the TID signal with a thread switch delay or overhead of one processor cycle.</p><p>Other processors may include other signals that generate a thread switch signal (TID). For example, some processors may be configured to switch threads on an L2 cache miss.</p><p>The thread switch logic <b>610</b> generates signals to allow the processor to switch context to another thread in response to an L1 cache load miss which pulses the L1_load miss terminal, and in response to an L1 cache instruction miss which pulses the L1_instruction_miss terminal. The thread switch logic <b>610</b> also generates signals allowing the processor to switch context to another thread when an instruction buffer is empty, generating a signal to the instruction_buffer_empty terminal. The thread switch logic <b>610</b> also switches context in response to external and internal interrupts which pulse the external_interrupt terminal and the internal_interrupt terminal, respectively.</p><p>The thread switch logic <b>610</b> permits control of thread selection based on priority of a particular thread via signals to the thread_priority terminal. The thread switch logic <b>610</b> is typically implemented as a control processor, microcontroller, microcode control logic, a logic circuit, or the like, all of which are well known in the electronics arts. Accordingly, fast thread switching upon an occurrence of an L1 cache miss may be selected.</p><p>In other applications, thread reservations may be selected on the basis of which process or context is the busiest. In these applications, the thread switch logic <b>610</b> implements a thread reservation system or thread locking system in which a thread pathway is reserved for usage by a selected thread. The thread switch logic <b>610</b> may select a particular thread that is to execute with priority in comparison to other threads. A high priority thread may be associated with an operation with strict time constraints, an operation that is frequently and predominantly executed in comparison to other threads. Thread switch logic <b>610</b> controls thread-switching operation so that a particular hardware thread is reserved for usage by the selected thread.</p><p>In one example of a priority operation, a JAVA\u2122 server typically includes a master process and a plurality of slave processes. The thread switch logic <b>610</b> reserves a hardware thread for the master process, which determines which slave process is permitted to execute at any time. The master process is therefore selected as a reserved thread that reserves a particular reserved hardware thread and is thus locked into the reserved hardware thread. The master process continues to execute in the reserved hardware thread but allocates nonreserved hardware threads among the plurality of slave threads.</p><p>In other applications, the thread switch logic <b>610</b> may be configured to support native threads in an operating system, for example Solaris native threads may be supported in Java applications.</p><p>Thread switch logic <b>610</b> includes an MT_mode terminal supplying multithreading mode signals to select particular threads for multi-processor execution.</p><p>The thread switch logic <b>610</b> supports a fast thread switch with a very small delay, for example three cycles or less. In some multithreading system and method embodiments, a processor performs a fast thread-switching operation in response to an L1 cache miss stall.</p><p>In other implementations, the thread switch logic <b>610</b> implements one or more of several thread-switching methods. A first thread-switching operation is \u201coblivious\u201d thread-switching for every N cycle in which the individual flip-flops locally determine a thread-switch without notification of stalling. The oblivious technique, typically implemented using a simple counter for counting cycles between switches, avoids usage of an extra global interconnection, such as wire or metal, between threads for thread selection.</p><p>A second thread-switching operation is \u201csemi-oblivious\u201d thread-switching for use with a load-use stall or \u201cpipeline stall\u201d signal. The pipeline stall signal operates in two capacities, first as a notification of a pipeline stall, and second as a thread select signal between threads so that, again, usage of an extra global interconnection between threads for thread selection is avoided. One suitable implementation of the semi-oblivious thread-switching technique employs a connection with a load/store unit global stall signal (lsu_stall_e) in UltraSPARC-I and UltraSPARC-II processors.</p><p>A third thread-switching operation is an \u201cintelligent global scheduler\u201d thread-switching in which a thread switch decision is selectively programmed, based on one or more signals. In one example an intelligent global scheduler uses signals such as: (1) an L1 data cache miss stall signal, (2) an L1 load miss signal, (3) an instruction buffer empty signal, (4) an instruction queue empty signal, (5) an L2 cache miss signal, (6) a thread priority signal, (7) a thread timer signal, (8) an interrupt signal, or other sources of triggering. In some embodiments, the thread select signal is broadcast as fast as possible, similar to a clock tree distribution. In some systems, a processor derives a thread select signal that is applied to the flip-flops by overloading a scan enable (SE) signal of a scannable flip-flop.</p><p>Various processor implementations include a thread switch logic <b>610</b> that segregates a cache in a processor into a plurality of N cache parts. Referring to FIG. 7A, a schematic block diagram shows an example of a cache <b>700</b> which is segregated into a first segregated cache portion <b>710</b> and a second segregated cache portion <b>712</b>, each of which include a plurality of storage regions. In one example, the first segregated cache portion <b>710</b> and the second segregated cache portion <b>712</b> each have a capacity of 8 kB, segregating a 16 kB directed-mapped 32-byte line cache. Although the illustrative example shows the cache <b>700</b> segregated into two equally-sized portions, other examples may segregate a cache into any number of portions. The cache portions may be equal in size or differing in size. Referring to FIG. 7B, a pictorial diagram shows an example of an addressing technique for the segregated cache <b>700</b>. A 64-bit virtual address storage <b>720</b> includes a cache virtual address tag bit field <b>722</b> with virtual address bits [<b>63</b>:<b>13</b>], a thread ID (TID) and index bits field <b>724</b> with index bits [<b>12</b>:<b>5</b>], and a byte offset bit field <b>726</b> with virtual address bits [<b>4</b>:<b>0</b>]. The index bits field <b>724</b> segregates the cache into two thread portions, the first thread segregated cache portion <b>710</b> and the second thread segregated cache portion <b>712</b>.</p><p>Cache segregation is selectively applied in a single cache or a plurality of caches including L1 caches, L2 caches, external caches, and the like. Cache segregation avoids interference, \u201ccross-talk\u201d, or \u201cpollution\u201d between threads. One technique for cache segregation utilizes logic for storing and communicating thread identification (TID) bits. The cache utilizes cache indexing logic. For example, the TID bits can be inserted at the most significant bits of the cache index. Sharing of an L2 cache among threads is easily-implemented since the L2 cache is physically indexed.</p><p>Various processor implementations include an anti-aliasing logic, shown in FIG. 8, which is coupled to a cache <b>810</b>, such as an L1 cache, and L2 cache, or others. For example, a processor may include anti-aliasing logic coupled to an L1 cache so that the L1 cache is shared among threads via anti-aliasing. In an illustrative example, the cache is a 16 kB direct-mapped virtually-indexed, physically-tagged (VIPT) cache <b>810</b> that is shared among threads. The cache <b>810</b> is addressed using a 64-bit virtual address storage <b>820</b> including cache virtual address tag bit field <b>822</b>, a cache index <b>812</b>, and byte offset bit field <b>826</b>. The cache index <b>812</b> is configured to include a thread ID (TID) <b>823</b> and index bits field <b>824</b>.</p><p>The anti-aliasing logic stores data to the same physical address (PA), shown as PA=B, from two different threads that map to two different indexed entries n and n+256. The anti-aliasing logic operates during a store to the cache <b>810</b> by comparing the physical address at the two indexed entries. If the physical addresses match, the duplicate entry is either invalidated or updated. The anti-aliasing logic avoids hazards that result from multiple virtual addresses mapping to one physical address. The anti-aliasing logic selectively invalidates or updates duplicate L1 cache entries.</p><p>Various processor implementations include native thread logic associated with the thread switch logic <b>610</b> and the anti-aliasing logic that supports lightweight processes and native threads. The logic supporting native threads and lightweight processes includes logic that disables thread ID tagging and disables cache segregation since lightweight processes and native threads share the same virtual address space. A lightweight process is a simplified, minimal-context process or thread that is typically designed to efficiently execute kernel functions. The lightweight process has very little context so is easily and efficiently switched into and out of execution. The lightweight process thus executes efficiently without thread ID tagging and cache segregation. The thread switch logic <b>610</b> accommodates lightweight processes by disabling thread ID tagging and cache segregation, advantageously avoiding allocation of cache and other resources to threads that do not utilize the resources.</p><p>Referring to FIG. 9, a schematic functional block diagram depicts a design configuration for a single-chip dual-processor vertically-threaded processor <b>900</b> that is suitable for implementing various multithreading techniques and system implementations that improve multithreading performance and functionality. The single-processor vertically-threaded processor <b>300</b> shown in FIG. 3 executes a vertical multithreading alone. In contrast, the single-chip dual-processor vertically-threaded processor <b>900</b> executes with both horizontal multithreading and vertical multithreading. The single-chip dual-processor vertically-threaded processor <b>900</b> has two processors on a single chip including a first vertical multithreading processor <b>902</b> and a second vertical multithreading processor <b>904</b>.</p><p>The first vertical multithreading processor <b>902</b> includes a thread <b>0</b> machine state block <b>910</b> that defines a machine state of a first thread (thread <b>0</b>) and a thread <b>1</b> machine state block <b>912</b> that defines a machine state of a second thread (thread <b>1</b>) that \u201cshadows\u201d the machine state of thread <b>0</b>. The thread <b>0</b> machine state block <b>910</b> and the thread <b>1</b> machine state block <b>912</b> have the single integrated circuit logic structure using high-speed multi-bit flip-flop design and four-dimensional register file structure, and supply instructions from thread <b>0</b> and thread <b>1</b> to a shared processor pipeline <b>914</b> using vertical threading. The shared processor pipeline <b>914</b> is connected to a dual load/store unit including a thread <b>0</b> load/store unit <b>916</b> and a thread <b>1</b> load/store unit <b>918</b> that execute load and store data accesses for instruction threads <b>0</b> and <b>1</b>, respectively.</p><p>The shared processor pipeline <b>914</b> and the dual load/store unit are connected to a shared data cache and a shared data memory management unit (DMMU). The shared data cache is used to cache data for both thread <b>0</b> and thread <b>1</b> computations.</p><p>The second vertical multithreading processor <b>904</b> includes a thread <b>2</b> machine state block <b>940</b> that defines a machine state of a third thread (thread <b>2</b>) and a thread <b>3</b> machine state block <b>942</b> that defines a machine state of a fourth thread (thread <b>3</b>) that \u201cshadows\u201d the machine state of thread <b>2</b>. The thread <b>2</b> machine state block <b>940</b> and the thread <b>3</b> machine state block <b>942</b> have the single integrated circuit logic structure using high-speed multi-bit flip-flop design and four-dimensional register file structure, and supply instructions from thread <b>2</b> and thread <b>3</b> to a shared processor pipeline <b>944</b> using vertical threading. The shared processor pipeline <b>944</b> is connected to a dual load/store unit including a thread <b>2</b> load/store unit <b>946</b> and a thread <b>3</b> load/store unit <b>948</b> that execute load and store data accesses for instruction threads <b>0</b> and <b>1</b>, respectively.</p><p>The shared processor pipeline <b>944</b> and the dual load/store unit are connected to a shared data cache and a shared data memory management unit (DMMU). The shared data cache is used to cache data for both thread <b>2</b> and thread <b>3</b> computations.</p><p>An instruction control block <b>960</b> includes an instruction (L1) cache, a branch prediction unit, NFRAM, and an instruction memory management unit (IMMU) all of which are shared between the multiple threads, thread <b>2</b> and thread <b>3</b>.</p><p>The two dual load/store units are also connected to an external cache control unit (ECU) <b>922</b>, which is connected to an external cache bus <b>924</b>. The external cache control unit <b>922</b> is also connected to an UltraPort Architecture Interconnect (UPA) bus <b>926</b> via a memory interface unit (MIU) <b>928</b>. The external cache control unit <b>922</b> and the memory interface unit (MIU) <b>928</b> are unified between four threads, thread <b>0</b>, thread <b>1</b>, thread <b>2</b>, and thread <b>3</b> to perform functions of cache miss processing and interfacing with external devices to supply, in combination, a plurality of execution threads to the thread <b>0</b> machine state block <b>910</b>, the thread <b>1</b> machine state block <b>912</b>, the thread <b>2</b> machine state block <b>940</b>, and the thread <b>3</b> machine state block <b>942</b> via a shared instruction control blocks <b>930</b> and <b>960</b>. The unified external cache control unit <b>922</b> and memory interface unit (MIU) <b>928</b> include thread identifier (TID) tagging to specify and identify the thread that is accessed via the external cache bus <b>924</b> and the UPA bus <b>926</b>.</p><p>The unified external cache control unit <b>922</b> and memory interface unit (MIU) <b>928</b> perform operations of cache misprocessing and interfacing with external devices. Misprocessing for a thread (a virtual CPU) takes place when the thread is inactive. In addition, multiprocessing is simplified when a multithread processor operates in the manner of a single processor to an external device. Therefore, in some processors the unified external cache control unit <b>922</b> and memory interface unit (MIU) <b>928</b> are shared structures with logical enhancements to support multiple threads but do not use flip-flops to duplicate ECU and MIU functionality for each thread.</p><p>The external cache bus <b>924</b> and the UPA bus <b>926</b> interfaces are shared between threads using a single port identifier.</p><p>The external cache control unit <b>922</b> manages instruction (L1) cache and data cache misses in both the first vertical multithreading processor <b>902</b> and the second vertical multithreading processor <b>904</b>, and permits up to one access every other cycle to the external cache. The external cache control unit <b>922</b> supports DMA accesses which hit in the external cache and maintains data coherence between the external cache and the main memory (not shown). The memory interface unit (MIU) <b>928</b> controls transactions to the UPA bus <b>926</b>.</p><p>The single-chip dual-processor vertically-threaded processor <b>900</b> also includes an on-chip L2 cache tag RAM <b>938</b> to support a two-way external L2 cache.</p><p>The single-chip dual-processor vertically-threaded processor <b>900</b> reduces wasted cycle time resulting from stalling and idling, and increases the proportion of execution time, by supporting and implementing both vertical multithreading and horizontal multithreading. Vertical multithreading permits overlapping or \u201chiding\u201d of cache miss wait times. In vertical multithreading, multiple hardware threads share the same processor pipeline. A hardware thread is typically a process, a lightweight process, a native thread, or the like in an operating system that supports multithreading, such as a Solaris UNIX operating system. Horizontal multithreading is attained by utilizing a plurality of pipelines, increasing parallelism within the processor circuit structure. The single-chip dual-processor vertically-threaded processor <b>900</b> attains vertical multithreading within a single integrated circuit die that makes up a single-chip processor. To further increase system parallelism the single-chip dual-processor vertically-threaded processor <b>900</b> executes with horizontal multithreading using multiple processor cores formed in a single die. Advances in on-chip multiprocessor horizontal threading are gained as processor core sizes are reduced through technological advancements.</p><p>The illustrative processor <b>900</b> and other multithreaded processors described herein employ thread level parallelism and operates on multiple independent threads, possibly attaining a multiplicative factor of the performance of a processor having the same resources and clock rate but utilizing traditional non-thread parallelism.</p><p>Thread level parallelism is particularly useful for Java\u2122 applications which are bound to have multiple threads of execution. Java\u2122 methods including \u201csuspend\u201d, \u201cresume\u201d, \u201csleep\u201d, and the like include effective support for threaded program code. In addition, Java\u2122 class libraries are thread-safe to promote parallelism. (Java\u2122, Sun, Sun Microsystems and the Sun Logo are trademarks or registered trademarks of Sun Microsystems, Inc. in the United States and other countries. All SPARC trademarks, including UltraSPARC I and UltraSPARC II, are used under license and are trademarks of SPARC International, Inc. in the United States and other countries. Products bearing SPARC trademarks are based upon an architecture developed by Sun Microsystems, Inc.) Furthermore, the thread model of the multithreaded processor <b>900</b> and other described multithreaded processors supports a dynamic compiler which runs as one thread while a second thread is used by the current application. In the illustrative system, the compiler applies optimizations based on \u201con-the-fly\u201d profile feedback information while dynamically modifying the executing code to improve execution on each subsequent run. For example, a \u201cgarbage collector\u201d may be executed as a first thread, copying objects or gathering pointer information, while the application is executing as a second thread.</p><p>Referring to FIG. 10, a schematic functional block diagram shows an alternative design configuration for a single-processor vertically-threaded processor <b>1000</b> that is suitable for implementing various multithreading techniques and system implementations that improve multithreading performance and functionality. The single-processor vertically-threaded processor <b>1000</b> is two-way vertically threaded with a single processor but with dual thread pipelines in a die. In an illustrative embodiment, the pipeline is based on an UltraSPARC IIi design with a peripheral component interconnect (PCI) interface and executes up to a 600 MHz processor clock frequency. The single-processor vertically-threaded processor <b>1000</b> includes a thread <b>0</b> machine state block <b>1010</b> that defines a machine state of a first thread (thread <b>0</b>) and incorporates a processor pipeline. The thread <b>0</b> machine state and pipeline block <b>1010</b> is shadowed by a thread <b>1</b> machine state block <b>1012</b> that defines a machine state of a second thread (thread <b>1</b>). The thread <b>0</b> machine state and pipeline block <b>1010</b> and shadow thread <b>1</b> machine state block <b>1012</b> are formed in a single integrated circuit logic structure using the previously-described high-speed multi-bit flip-flop design and a \u201cfour-dimensional\u201d register file structure. The four-dimensional register file structure is formed in a plurality of layers of storage cells. The storage cell layers have a two-dimensional form including storage storing data bytes or words including a plurality of bits. Vertical threading introduces a fourth dimension since the three-dimensional register file is defined for a plurality of machine states that are duplicated for the registers. The multiple-dimension register file multiplicatively increases the register file storage capacity without changing the integrated circuit size since the size depends on the number and density of devices across the surface area of a semiconductor die. A suitable multiple-dimension register file is disclosed in more detail in U.S. Pat. No. 5,721,868, entitled \u201cRAPID REGISTER FILE ACCESS BY LIMITING ACCESS TO A SELECTABLE REGISTER SUBSET\u201d, issued Feb. 24, 1998 (Yung et al) which is incorporated by reference herein in its entirety.</p><p>The multiple-dimension register file structure is highly advantageous for increasing processor performance without increasing size, and for de