located and deallocated storage within the cache. Since many memory operations are being attempted, it becomes more likely that numerous cache misses will be experienced. Furthermore, in many common cases, one miss within a cache line may rapidly be followed by a large number of additional misses to that cache line. These misses may fill or come close to filling, the buffers allocated within the processor for memory operations. An efficient scheme for buffering memory operations is therefore needed.</p><p>An additional problem which becomes even more onerous as processors employ wider superscalar configurations and/or deeper pipelines is the issue of store to load forwarding. As more memory operations may be queued up prior to completion, it becomes more likely that load memory operations will hit prior store memory operations still in the buffers. Furthermore, as speculative instruction execution increases due to the larger number of instructions in-flight within the processor, it becomes more likely that loads will attempt to execute prior to the stores receiving their store data. While loads which hit older stores which have corresponding store data available may receive the corresponding store data from the buffers, loads which hit older stores for which corresponding store data is not available generally are rescheduled at a later time. As the amount of time to schedule the load, execute the load, access the data cache (and detect the hit on the store), and forward the data increases, the delay from the corresponding store data being provided to the data being forwarded as the load data tends to increase. Furthermore, deeper buffers tend to increase the amount of time between scheduling attempts (to allow other memory operations to be scheduled). Performance of the processor, which may be quite dependent on load latency, may therefore suffer. A mechanism for minimizing load delay for loads which hit stores for which store data is unavailable is therefore desired.</p><h4>SUMMARY OF THE INVENTION</h4><p>The problems outlined above are in large part solved by a processor employing a dependency link file as described herein. Upon detection of a load which hits a store for which store data is not available, the processor allocates an entry within the dependency link file for the load. The entry stores a load identifier identifying the load and a store data identifier identifying a source of the store data. The dependency link file monitors results generated by execution units within the processor to detect the store data being provided. The dependency link file then causes the store data to be forwarded as the load data in response to detecting that the store data is provided. The latency from store data being provided to the load data being forwarded may thereby be minimized. Particularly, the load data may be forwarded without requiring that the load memory operation be scheduled. Performance of the microprocessor may be increased due to the reduced load latency achievable in the above-mentioned cases.</p><p>Broadly speaking, a load/store unit is contemplated comprising a first buffer, first control logic coupled to the first buffer, second control logic, and a second buffer coupled to the second control logic. The first buffer comprises a first plurality of entries, each of the first plurality of entries being configured to store a store address and a corresponding store data of a respective store memory operation. The first control logic is configured to detect a first load memory operation having a first load address which hits a first store address within a first entry of the first plurality of entries and for which a first corresponding store data is not stored within the first entry. Coupled to receive a signal from the first control logic indicative of detecting the first load memory operation, the second control logic is configured to allocate a second entry of a second plurality of entries in the second buffer to the first load memory operation in response to the signal. The second entry is configured to store a first load identifier identifying the first load memory operation and a first store data identifier identifying a source of the first corresponding store data in response to the second control logic allocating the second entry.</p><p>A processor is contemplated comprising a data cache and a load/store unit. The load/store unit includes a first buffer comprising a plurality of entries. The load/store unit is configured to allocate a first entry of the plurality of entries to a first load memory operation in response to detecting that a first load address of the first load memory operation hits a first store address of a first store memory operation for which a first store data is not available during a probe of the first load memory operation to the data cache. The first entry stores a first load identifier identifying the first load memory operation and a first store data identifier identifying a source of the first store data. Additionally, a computer system is contemplated comprising the processor and an input/output (I/O) device. The I/O device provides communication between the computer system and another computer system to which the I/O device is coupled.</p><p>Moreover, a method for performing a load memory operation is contemplated. A data cache is probed with the load memory operation. The load memory operation is detected as hitting a store memory operation for which corresponding store data is not available during the probing. A load identifier identifying the load memory operation and a store data identifier identifying a source of the store data is recorded to a first buffer. The store data is detected as being provided by receiving the store data identifier from the source. The load identifier is forwarded from the first buffer and the store data is forwarded in response to detecting that the stored data is being provided.</p><?BRFSUM description=\"Brief Summary\" end=\"tail\"?><?brief-description-of-drawings description=\"Brief Description of Drawings\" end=\"lead\"?><h4>BRIEF DESCRIPTION OF THE DRAWINGS</h4><p>Other objects and advantages of the invention will become apparent upon reading the following detailed description and upon reference to the accompanying drawings in which:</p><p>FIG. 1 is a block diagram of one embodiment of a processor.</p><p>FIG. 2 is a block diagram of one embodiment of a decode unit, a reservation station, a functional unit, a reorder buffer, a load/store unit, a data cache, and a bus interface unit shown in FIG. <b>1</b>.</p><p>FIG. 3 is a block diagram of one embodiment of a load/store unit.</p><p>FIG. 4 is a timing diagram illustrating one embodiment of a data cache access pipeline from a first buffer (LS1) illustrated in FIG. <b>3</b>.</p><p>FIG. 5 is a timing diagram illustrating one embodiment of a data cache access pipeline from a second buffer (LS2) illustrated in FIG. <b>3</b>.</p><p>FIG. 6 is a block diagram of one embodiment of an entry within LS1.</p><p>FIG. 7 is a block diagram of one embodiment of an entry within LS2.</p><p>FIG. 8 is a circuit diagram illustrating a portion of one embodiment of selection logic for LS1.</p><p>FIG. 9 is a circuit diagram illustrating a portion of one embodiment of selection logic for LS2.</p><p>FIG. 10 is a timing diagram illustrating data forwarding with and without a dependency link file.</p><p>FIG. 11 is a block diagram of one embodiment of a load/store unit and a data cache, with the load/store unit employing a dependency link file.</p><p>FIG. 12 is a timing diagram illustrating exemplary loads and stores performed by two processors.</p><p>FIG. 13 is a block diagram of a portion of one embodiment of a load/store unit including snoop hardware.</p><p>FIG. 14 is a flowchart illustrating operation of one embodiment of control logic shown in FIG. 13 during a snoop request.</p><p>FIG. 15 is a flowchart illustration operation of one embodiment of control logic shown in FIG. 13 during a reprobe operation.</p><p>FIG. 16 is a diagram illustrating an example of snoop resync operation.</p><p>FIG. 17 is a block diagram of a portion of one embodiment of a load/store unit including self-modifying code checking hardware.</p><p>FIG. 18 is a state machine which may be employed by one embodiment of the load/store unit shown in FIG. <b>17</b>.</p><p>FIG. 19 is a block diagram of a portion of one embodiment of an instruction cache which may be used with the load/store unit shown in FIG. <b>17</b>.</p><p>FIG. 20 is a flowchart illustrating operation of one embodiment of self-modifying code check control logic shown in FIG. <b>10</b>.</p><p>FIG. 21 is a block diagram of a portion of one embodiment of a load/store unit including exponential backoff.</p><p>FIG. 22 is a state machine which may be employed by one embodiment of a bus interface unit including exponential backoff.</p><p>FIG. 23 is a flowchart illustrating operation of one embodiment of control logic illustrated in FIG. <b>21</b>.</p><p>FIG. 24 is a timing diagram illustrating an example of exponential backoff.</p><p>FIG. 25 is a block diagram of one embodiment of a computer system including the processor shown in FIG. <b>1</b>.</p><?brief-description-of-drawings description=\"Brief Description of Drawings\" end=\"tail\"?><?DETDESC description=\"Detailed Description\" end=\"lead\"?><p>While the invention is susceptible to various modifications and alternative forms, specific embodiments thereof are shown by way of example in the drawings and will herein be described in detail. It should be understood, however, that the drawings and detailed description thereto are not intended to limit the invention to the particular form disclosed, but on the contrary, the intention is to cover all modifications, equivalents and alternatives falling within the spirit and scope of the present invention as defined by the appended claims.</p><h4>DETAILED DESCRIPTION OF THE PREFERRED EMBODIMENTS</h4><h4>Processor Overview</h4><p>Turning now to FIG. 1, a block diagram of one embodiment of a processor <b>10</b> is shown. Other embodiments are possible and contemplated. As shown in FIG. 1, processor <b>10</b> includes a prefetch/predecode unit <b>12</b>, a branch prediction unit <b>14</b>, an instruction cache <b>16</b>, an instruction alignment unit <b>18</b>, a plurality of decode units <b>20</b>A-<b>20</b>C, a plurality of reservation stations <b>22</b>A-<b>22</b>C, a plurality of functional units <b>24</b>A-<b>24</b>C, a load/store unit <b>26</b>, a data cache <b>28</b>, a register file <b>30</b>, a reorder buffer <b>32</b>, an MROM unit <b>34</b>, and a bus interface unit <b>37</b>. Elements referred to herein with a particular reference number followed by a letter will be collectively referred to by the reference number alone. For example, decode units <b>20</b>A-<b>20</b>C will be collectively referred to as decode units <b>20</b>.</p><p>Prefetch/predecode unit <b>12</b> is coupled to receive instructions from bus interface unit <b>37</b>, and is further coupled to instruction cache <b>16</b> and branch prediction unit <b>14</b>. Similarly, branch prediction unit <b>14</b> is coupled to instruction cache <b>16</b>. Still further, branch prediction unit <b>14</b> is coupled to decode units <b>20</b> and functional units <b>24</b>. Instruction cache <b>16</b> is further coupled to MROM unit <b>34</b> and instruction alignment unit <b>18</b>. Instruction alignment unit <b>18</b> is in turn coupled to decode units <b>20</b>. Each decode unit <b>20</b>A-<b>20</b>C is coupled to load/store unit <b>26</b> and to respective reservation stations <b>22</b>A-<b>22</b>C. Reservation stations <b>22</b>A-<b>22</b>C are further coupled to respective functional units <b>24</b>A-<b>24</b>C. Additionally, decode units <b>20</b> and reservation stations <b>22</b> are coupled to register file <b>30</b> and reorder buffer <b>32</b>. Functional units <b>24</b> are coupled to load/store unit <b>26</b>, register file <b>30</b>, and reorder buffer <b>32</b> as well. Data cache <b>28</b> is coupled to load/store unit <b>26</b> and to bus interface unit <b>37</b>. Bus interface unit <b>37</b> is further coupled to an L2 interface to an L2 cache and a bus. Finally, MROM unit <b>34</b> is coupled to decode units <b>20</b>.</p><p>Instruction cache <b>16</b> is a high speed cache memory provided to store instructions. Instructions are fetched from instruction cache <b>16</b> and dispatched to decode units <b>20</b>. In one embodiment, instruction cache <b>16</b> is configured to store up to 64 kilobytes of instructions in a 2 way set associative structure having 32 byte lines (a byte comprises 8 binary bits). Alternatively, any other desired configuration and size may be employed. For example, it is noted that instruction cache <b>16</b> may be implemented as a fully associative, set associative, or direct mapped configuration.</p><p>Instructions are stored into instruction cache <b>16</b> by prefetch/predecode unit <b>12</b>. Instructions may be prefetched prior to the request thereof from instruction cache <b>16</b> in accordance with a prefetch scheme. A variety of prefetch schemes may be employed by prefetch/predecode unit <b>12</b>. As prefetch/predecode unit <b>12</b> transfers instructions to instruction cache <b>16</b>, prefetch/predecode unit <b>12</b> generates three predecode bits for each byte of the instructions: a start bit, an end bit, and a functional bit. The predecode bits form tags indicative of the boundaries of each instruction. The predecode tags may also convey additional information such as whether a given instruction can be decoded directly by decode units <b>20</b> or whether the instruction is executed by invoking a microcode procedure controlled by MROM unit <b>34</b>, as will be described in greater detail below. Still further, prefetch/predecode unit <b>12</b> may be configured to detect branch instructions and to store branch prediction information corresponding to the branch instructions into branch prediction unit <b>14</b>. Other embodiments may employ any suitable predecode scheme.</p><p>One encoding of the predecode tags for an embodiment of processor <b>10</b> employing a variable byte length instruction set will next be described. A variable byte length instruction set is an instruction set in which different instructions may occupy differing numbers of bytes. An exemplary variable byte length instruction set employed by one embodiment of processor <b>10</b> is the x86 instruction set.</p><p>In the exemplary encoding, if a given byte is the first byte of an instruction, the start bit for that byte is set. If the byte is the last byte of an instruction, the end bit for that byte is set. Instructions which may be directly decoded by decode units <b>20</b> are referred to as \u201cfast path\u201d instructions. The remaining x86 instructions are referred to as MROM instructions, according to one embodiment. For fast path instructions, the functional bit is set for each prefix byte included in the instruction, and cleared for other bytes. Alternatively, for MROM instructions, the functional bit is cleared for each prefix byte and set for other bytes. The type of instruction may be determined by examining the functional bit corresponding to the end byte. If that functional bit is clear, the instruction is a fast path instruction. Conversely, if that functional bit is set, the instruction is an MROM instruction. The opcode of an instruction may thereby be located within an instruction which may be directly decoded by decode units <b>20</b> as the byte associated with the first clear functiona