g units to carry out program instructions, and firmware 18 whose primary purpose is to seek out and load an operating system from one of the peripherals (usually the permanent memory device) whenever the computer is first turned on. Processing units 12a and 12b communicate with the peripheral devices by various means, including a generalized interconnect or bus 20. Computer system 10 may have many additional components which are not shown, such as serial and parallel ports for connection to, e.g., modems or printers. Those skilled in the art will further appreciate that there are other components that might be used in conjunction with those shown in the block diagram of FIG. 1; for example, a display adapter might be used to control a video display monitor, a memory controller can be used to access memory 16, etc. Instead of connecting I/O devices 14 directly to bus 20, they may be connected to a secondary (I/O) bus which is further connected to an I/O bridge to bus 20. The computer also can have more than two processing units.</p><p>In a symmetric multi-processor (SMP) computer, all of the processing units are generally identical; that is, they all use a common set or subset of instructions and protocols to operate and generally have the same architecture. A typical architecture is shown in FIG. 1. A processing unit includes a processor core 22 having a plurality of registers and execution units, which carry out program instructions in order to operate the computer. An exemplary processing unit includes the PowerPC\u2122 processor marketed by International Business Machines Corporation. The processing unit also can have one or more caches, such as an instruction cache 24 and a data cache 26, which are implemented using high-speed memory devices. Instructions and data may be directed to the respective cache 24 or 26 by examining a signal that is indicative of whether the CPU is requesting an operation whose operand is instruction versus data. Caches are commonly used to temporarily store values that might be repeatedly accessed by a processor, in order to speed up processing by avoiding the longer step of loading the values from memory 16. These caches are referred to as \"on-board\" when they are integrally packaged with the processor core on a single integrated chip 28. Each cache is associated with a cache controller (not shown) that manages the transfer of data between the processor core and the cache memory.</p><p>A processing unit 12 can include additional caches, such as cache 30, which is referred to as a level 2 (L2) cache since it supports the on-board (level 1) caches 24 and 26. In other words, cache 30 acts as an intermediary between memory 16 and the on-board caches, and can store a much larger amount of information (instructions and data) than the on-board caches can, but at a longer access penalty. For example, cache 30 may be a chip having a storage capacity of 256 or 512 kilobytes, while the processor may be an IBM PowerPC\u2122 604-series processor having on-board caches with 64 kilobytes of total storage. Cache 30 is connected to bus 20, and all loading of information from memory 16 into processor core 22 must come through cache 30. Although FIG. 1 depicts only a two-level cache hierarchy, multi-level cache hierarchies can be provided where there are many levels of serially connected caches.</p><p>A cache has many \"blocks\" which individually store the various instructions and data values. The blocks in any cache are divided into groups of blocks called \"sets.\" A set is the collection of cache blocks that a given memory block can reside in. For any given memory block, there is a unique set in the cache that the block can be mapped into, according to preset mapping functions. The number of blocks in a set is referred to as the associativity of the cache, e.g., 2-way set associative means that for any given memory block, there are two blocks in the cache that the memory block can be mapped into; however, several different blocks in main memory can be mapped to any given set. A 1-way set associative cache is direct mapped; that is, there is only one cache block that can contain a particular memory block. A cache is said to be fully associative if a memory block can occupy any cache block, i.e., there is one set, and the address tag is the full address of the memory block.</p><p>An exemplary cache line (block) includes an address-tag field, a state-bit field, an inclusivity-bit field, and a value field for storing the actual instruction or data. The state-bit field and inclusivity-bit fields are used to maintain cache coherency in a multiprocessor computer system. The address tag is a subset of the full address of the corresponding memory block. A compare match of an incoming effective address with one of the tags within the address-tag field indicates a cache \"hit.\" The collection of all of the address tags in a cache (and sometimes the state bit and inclusivity bit fields) is referred to as a directory, and the collection of all of the value fields is the cache entry array.</p><p>When all of the blocks in a set for a given cache are full and that cache receives a request, whether a \"read\" or \"write,\" to a memory location that maps into the full set, the cache must \"evict\" one of the blocks currently in the set. The cache chooses a block by one of a number of means known to those skilled in the art (least recently used (LRU), random, pseudo-LRU, etc.) to be evicted. If the data in the chosen block is modified, that data is written to the next lowest level in the memory hierarchy which may be another cache (in the case of the L1 or on-board cache) or main memory (in the case of an L2 cache, as depicted in the two-level architecture of FIG. 1). By the principle of inclusion, the lower level of the hierarchy will already have a block available to hold the written modified data. However, if the data in the chosen block is not modified, the block is simply abandoned and not written to the next lowest level in the hierarchy. This process of removing a block from one level of the hierarchy is known as an \"eviction.\" At the end of this process, the cache no longer holds a copy of the evicted block.</p><p>Some procedures (programs) running on a processor have the unintended effect of repeatedly using a limited number of sets (congruence classes) such that the cache is less efficient. In other words, when a procedure causes a large number of evictions in a small number of congruence class members while not using a large number of other members, there are increased memory latency delays. This effect, referred to as a stride, is related to the congruence mapping function and the manner in which the particular procedure is allocating memory blocks in the main memory device (RAM 16). The statistical advantages of using a particular associative cache break down for these types of procedures.</p><p>Another statistical advantage which sometimes breaks down relates to the provision of separate cache blocks (such as caches 24 and 26) for instructions and data. A typical processing unit provides an equal number of L1 cache blocks for instruction and data, so 50% of the available cache entries can be used at this level for instructions and 50% can be used for data. In the L2 cache, there is no differentiation, i.e., 100% of the cache at the L2 level is available for instructions and 100% is available for data. This ratio of available blocks for instruction versus data is not, however, always the most efficient usage of the cache for a particular procedure. Many software applications will perform better when run on a system with split I/D caching, while others perform better when run on a flat, unified cache (given the same total cache space). In the instances where the cache I/D ratio is not particularly close to the actual ratio of instruction and data cache operations, there are again a troubling number of evictions.</p><p>Yet another statistical advantage of an associative cache that may break down relates to the cache replacement algorithm which determines which cache block in a given set will be evicted. For example, an 8-way associative cache might use an LRU unit which examines a 7-bit field associated with the set. Due to a particular cycling frequency of the procedure running on the processor, this 7-bit LRU algorithm might result in evicting a higher number of cache blocks than might occur if the cache were 4-way associative, or 2-way associative.</p><p>It is difficult to statistically optimize associative caches because different technical applications may present different stride conditions or different instruction/data ratios. For example, a desktop publishing program, a warehouse inventory program, an aerodynamics modelling program and a server program might all present different stride conditions or ratios of instruction operations to data operations. It would, therefore, be desirable and advantageous to design a cache which can more fully optimize its statistical advantages regardless of the type of procedure running on the processor.</p><h4>SUMMARY OF THE INVENTION</h4><p>It is therefore one object of the present invention to provide an improved cache for a processor of a computer system.</p><p>It is another object of the present invention to provide such a cache which optimizes statistical advantages with respect to associativity.</p><p>It is yet another object of the present invention to provide such a cache which optimizes statistical advantages with respect to accessing instructions versus data.</p><p>It is still another object of the present invention to provide such a cache which optimizes statistical advantages with respect to a cache replacement (eviction) algorithm.</p><p>The foregoing objects are achieved in a method of providing associativity in a cache used by a processor of a computer system, generally comprising the steps of defining a congruence class of a memory block in the memory device using a first mapping function, the congruence class providing a first associativity level of the cache, loading program instructions in the processor for selecting a second associativity level of a known appropriate level, and implementing the second associativity level in the cache using a second mapping function. Application software may provide the program instructions, wherein the application software has procedures that may result in cache \"strides\" at particular associativity levels, and the known appropriate level is chosen to lessen memory latencies due to strides. Alternatively, the program instructions may be part of an operating system which monitors memory address requests, determines how efficiently a procedure will operate at different associativity levels, and selects a most efficient level for the known appropriate level. The congruence class is defined by associating the memory block with a particular set of cache blocks in the cache, based on a first portion of an address of the memory block, and the second associativity level is implemented by dividing the particular set into subsets and selecting a subset for the memory block based on a second portion of the address. The program instructions can monitor the cache misses as the cache uses the second associativity level, re-select the first associativity level based on the cache misses, and implement the first associativity level in the cache using the first mapping function again. The program instructions thereby can dynamically select between associativity levels using different mapping functions based on current operating conditions of the cache. The program instructions may select the associativity level by setting a value in a bit facility corresponding to the desired mapping function.</p><p>The above as well as additional objectives, features, and advantages of the present invention will become apparent in the following detailed written description.</p><?BRFSUM description=\"Brief Summary\" end=\"tail\"?><?brief-description-of-drawings description=\"Brief Description of Drawings\" end=\"lead\"?><h4>BRIEF DESCRIPTION OF THE DRAWINGS</h4><p>The novel features believed characteristic of the invention are set forth in the appended claims. The invention itself, however, as well as a preferred mode of use, further objectives, and advantages thereof, will best be understood by reference to the following detailed description of an illustrative embodiment when read in conjunction with the accompanying drawings, wherein:</p><p>FIG. 1 is a block diagram of a prior-art multi-processor computer system;</p><p>FIGS. 2a-2c are representations of a novel method of varying associativity for an associative cache;</p><p>FIG. 3 depicts one method of providing programmable associativity such as that shown in FIGS. 2a-2c, using a basic congruence class mapping that is modified by creating additional classes by using bits from the address tag;</p><p>FIG. 4 depicts a novel method of providing programmable congruence classes allowing arbitrary assignment of particular addresses to particular congruency classes, by switching address bi