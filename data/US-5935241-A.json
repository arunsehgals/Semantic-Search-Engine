tion of earlier instructions. Because of pipelining, the effective rate at which instructions are executed by a microprocessor can approach one instruction per machine cycle in a single pipeline microprocessor, even though the processing of each individual instruction may require multiple machine cycles from fetch through execution. So-called superscalar architectures effectively have multiple pipelines operating in parallel, providing even higher theoretical performance levels.</p><p>Of course, as is well known in the art, branching instructions are commonplace in most conventional computer and microprocessor programs. Branching instructions are instructions that alter the program flow, such that the next instruction to be executed after the branching instruction is not necessarily the next instruction in program order. Branching instructions may be unconditional, such as JUMP instructions, subroutine calls, and subroutine returns. Some branching instructions are conditional, as the branch depends upon the results of a previous logical or arithmetic instruction.</p><p>Conditional branching instructions present complexity in microprocessors of pipelined architecture, because the condition upon which the branch depends is not known until execution, which may be several cycles after fetch. In these situations, the microprocessor must either cease fetching instructions after the branch until the condition is resolved, in a \"bubble\" of empty stages (i.e., potential instruction processing slots) into the pipeline, or must instead speculatively fetch an instruction (in effect guessing the condition) in order to keep the pipeline full, at a risk of having to \"flush\" the pipeline of its current instructions if the speculation is determined to be incorrect.</p><p>The benefit of speculative execution of instructions in keeping the pipeline full, particularly in architectures with long or multiple pipelines, typically outweighs the performance degradation of pipeline flushes, so long as the success rate of the speculative execution is sufficient to achieve the desired performance benefit. Many modern microprocessors therefore follow some type of branch prediction techniques by way of which the behavior of conditional branching instructions may be predicted with some accuracy. One type of branch prediction is referred to as \"static\" prediction, as the prediction does not change over time or history. A simple static prediction approach merely predicts all conditional branches to be \"taken\". An improved static branch prediction approach predicts according to branch direction, for example by predicting all conditional branches in the forward direction to be \"not taken\" and predicting all conditional backward branches (e.g., LOOP instructions in DO loops) to be \"taken\". Of course, unconditional branches may always be statically predicted as \"taken\".</p><p>Dynamic branch prediction refers to a known technique of branch prediction that uses the results of past branches to predict the result of the next branch. A simple well-known dynamic prediction technique merely uses the results of the most recent one or two conditional branching instructions to predict the direction of a current branching instruction.</p><p>A more accurate dynamic branch prediction approach predicts the direction of a branching instruction by its own branching history, as opposed to the branch results of other instructions. This approach is generally incorporated into modem microprocessors by way of a branch target buffer. A conventional branch target buffer, or BTB, is a cache-like table of entries that each store an identifier (a \"tag\") for recently-encountered branching instructions, a branch history-related code upon which prediction is made, and a target address of the next instruction to be fetched if the branch is predicted as taken (the next sequential address being the address to be fetched for a \"not taken\" prediction). When a branching instruction is fetched, its address is matched against the tags in the BTB to determine if this instruction has been previously encountered; if so, the next instruction is fetched according to the prediction code indicated in the BTB for that instruction. Newly-encountered branching instructions are statically predicted, as no history is present in the BTB. Upon execution and completion of the instruction, the BTB entry is created (typically, for taken branches only) or modified (for branches already having a BTB entry) to reflect the actual result of the branching instruction, for use in the next occurrence of the instruction.</p><p>Various conventional alternative actual prediction algorithms that predict branches based upon the most recently executed branches or upon the branching history of the same instruction, are known in the art. A well-known simple prediction algorithm follows a four-state state machine model, and uses the two most recent branch events to predict whether the next occurrence will be taken or not taken. The four states are referred to as \"strongly taken\", \"taken\", \"not taken\", and \"strongly not taken\". A \"strongly\" state corresponds to at least the last two branches (either generally or for the particular instruction, depending upon the implementation) having been taken or not taken, as the case may be. The taken and not taken states (i.e., not a \"strongly\" state) correspond to the last two branches having differing results, with the next branch result either changing the prediction to the other result, or maintaining the prediction but in a \"strongly\" state.</p><p>A recent advance in branch prediction algorithms uses not only branch history results, but also branch pattern information, in generating a prediction of branch behavior. For example, a certain branch instruction may be a loop of three passes, such that its branch history will repetitively follow a pattern of taken-taken-not taken. Use of a simple two-bit, or four-state, prediction mechanism will not correctly predict the branching of this instruction, even though its behavior is entirely predictable. The well-known two-level adaptive branch prediction mechanism, described in Yeh &amp; Patt, \"Two-Level Adaptive Branch Prediction\", Proceedings of the 24<sup>th</sup> International Symposium on Microarchitecture, (ACM/IEEE, November 1991), pp. 51-61, uses both branch history and branch pattern information to predict the results of a branching instruction. Branch prediction using the Yeh &amp; Patt approach has been applied to microprocessor architectures using BTBs, as described in U.K. Patent Application 2 285 526, published Jul. 12, 1995. Attention is also directed, in this regard, to U.S. Pat. No. 5,574,871.</p><p>According to the approach described in the above-referenced Yeh and Patt paper and U.K. Patent Application 2 285 526, a pattern history is maintained and updated for each unique branch pattern. In this approach, the pattern history consists of the four-state state machine model described above, in which the two most recent branch events for each branch pattern predicts whether the next occurrence of a branch having the same branch pattern will be taken or not taken (along with its \"strongly\" attribute). In operation, upon detection of a branching instruction having an entry in the BTB, the branch pattern contained in the branch history field for that instruction indexes into the pattern history table, from which the prediction is obtained. Upon resolution of the branch, both the branch history field for the particular instruction and the pattern history for its previous pattern (i.e., the branch pattern used in the prediction) are updated. The updated pattern history is then available for use in predicting the outcome of the next branch instruction having its associated branch pattern in its branch history field of the BTB. The pattern history table according to this approach is thus \"global\", in the sense that the branch prediction is generated for any branch instruction having the same branch history pattern, regardless of the identity of the instruction. Accordingly, the pattern history for a particular branch pattern will be defined and updated based upon the branch prediction results for any branching instruction having that branch history. The branch prediction for any given instruction will thus be determined based upon the branch results of other, dissimilar, instructions, according to this basic two-level technique.</p><p>As described in Yeh and Patt, \"Alternative Implementations of Two-Level Adaptive Branch Prediction\", Conference Proceedings of the 19<sup>th</sup> Annual International Symposium on Computer Architecture, (ACM, May 1992), pp. 124-134, an alternative implementation of two-level branch prediction addresses this limitation. This alternative implementation provides address-specific pattern history tables, such that each entry in the BTB has its own pattern history table, as shown in FIG. 3 of this paper. Accordingly, the branch prediction for a branching instruction is made based upon the pattern history as generated and modified by its own past history, and is not dependent upon the branch results for other branching instructions having similar branch patterns.</p><p>While the use of address-specific pattern history tables eliminates interference in the branch prediction from other branching instructions having the same branch patterns, the cost of implementation can be quite substantial. For example, modern microprocessors may have BTBs with up to as many as 4 k entries. The use of an index of four bits of branch history into address-specific pattern history tables thus requires 4 k pattern history tables, each with sixteen entries that are two bits in width, resulting in 128 kbits of storage. The chip area required for implementation of this approach is thus quite substantial. This cost rapidly increases, however, as branch prediction is sought to be improved through the use of additional branch history bits as the index to the pattern history tables; for example, the use of six branch history bits would require 512 kbits of pattern history storage. As microprocessors continue to have more pipelines, each deeper in stages, resulting in more severe penalties for branch misprediction and thus a higher premium on accurate branch prediction, the cost of implementing address-specific pattern history tables becomes even greater.</p><p>By way of further background, it has been observed that microprocessor programs of different types have similarities in branch behavior within the type, and dissimilarities across types. For example, as described in Calder and Grunwald, \"The Predictability of Branches in Libraries\", Proceedings of the 28<sup>th</sup> International Symposium on Microarchitecture (ACM/IEEE, November 1995), pp. 24-34, commonly used UNIX library subroutines tend to have predictable branching behavior and, as a class or type, different branching behavior from non-library programs.</p><p>By way of further background, indexing into a global pattern history table using both branch history and a portion of the tag field of the BTB is known.</p><h4>BRIEF SUMMARY OF THE INVENTION</h4><p>It is therefore an object of the present invention to provide branch prediction in a microprocessor that is based upon program type.</p><p>It is a further object of the present invention to provide such branch prediction in which dissimilar branching instructions do not modify branch pattern history tables.</p><p>It is a further object of the present invention to provide such branch prediction in which similar branching instructions can set the branch pattern history for a branching instruction that is newly encountered, thus improving branch prediction for first instances of branching instructions.</p><p>Other objects and advantages of the present invention will be apparent to those of ordinary skill in the art having reference to the following specification together with its drawings.</p><p>The present invention may be implemented into a microprocessor by providing multiple global pattern history tables. The pattern history tables are global in the sense that the pattern history for each entry is generated and updated by the results of multiple branching instructions. The multiple pattern history tables each correspond to branching instructions from different sources, for example from supervisor or user code in an x86 architecture microprocessor, from instructions residing in selected windows of memory space, from the state of a global bit in a page table entry for the page frame in memory from which the instruction was fetched, or from other control information corresponding to the type of program containing the instruction. Each of the global pattern history tables thus provides prediction information for branching instructions from its type of program, to take advantage of the similarity of branch behavior for programs of similar types and to reduce interference from dissimilar branch behavior from branching instructions of programs of different types.</p><?BRFSUM description=\"Brief Summary\" end=\"tail\"?><?brief-description-of-drawings description=\"Brief Description of Drawings\" end=\"lead\"?><h4>BRIEF DESCRIPTION OF THE SEVERAL VIEWS OF THE DRAWING</h4><p>FIG. 1 is an electrical diagram, in block form, of a microprocessor and system constructed according to the preferred embodiments of the invention.</p><p>FIG. 2 is an electrical diagram, in block form, of the fetch unit in the microprocessor of FIG. 1 according to the preferred embodiments of the invention.</p><p>FIG. 3 is an electrical diagram, in block and schematic form, of the branch target buffer, pattern history tables, and associated circuitry in the microprocessor of FIG. 1 according to a first preferred embodiment of the invention.</p><p>FIG. 4 is an electrical diagram, in block and schematic form, of the branch target buffer, pattern history tables, and associated circuitry in the microprocessor of FIG. 1 according to a second preferred embodiment of the invention.</p><p>FIG. 5 is an electrical diagram, in block and schematic form, of the branch target buffer, pattern history tables, and associated circuitry in the microprocessor of FIG. 1 according to a third preferred embodiment of the invention.</p><p>FIG. 6 is an electrical diagram, in block and schematic form, of the branch target buffer, pattern history tables, and associated circuitry in the microprocessor of FIG. 1 according to a fourth preferred embodiment of the invention.</p><p>FIG. 7 is an electrical diagram, in block and schematic form, of a branch history buffer, pattern history tables, and associated circuitry in the microprocessor of FIG. 1 according to a fifth preferred embodiment of the invention.</p><?brief-description-of-drawings description=\"Brief Description of Drawings\" end=\"tail\"?><?DETDESC description=\"Detailed Description\" end=\"lead\"?><h4>DETAILED DESCRIPTION OF THE INVENTION</h4><p>Referring now to FIG. 1, an exemplary data processing system 300, including an exemplary superscalar pipelined microprocessor 10 within which the preferred embodiment of the invention is implemented, will be described. It is to be understood that the architecture of system 300 and of microprocessor 10 is described herein by way of example only, as it is contemplated that the present invention may be utilized in microprocessors of various architectures. It is therefore contemplated that one of ordinary skill in the art, having reference to this specification, will be readily able to implement the present invention in such other microprocessor architectures. It is further contemplated that the present invention may be realized in single-chip microprocessors and microcomputers or in multiple-chip implementations, with the manufacture of such integrated circuits accomplished according to silicon substrate, silicon-on-insulator, gallium arsenide, and other manufacturing technologies, and using MOS, CMOS, bipolar, BiCMOS, or other device implementations.</p><p>Microprocessor 10, as shown in FIG. 1, is connected to other system devices by way of external bus BUS. While external bus BUS, in this example, is shown as a single bus, it is of course contemplated that external bus BUS may represent multiple buses having different speeds and protocols, as is known in conventional computers utilizing the PCI local bus architecture. System 300 contains such conventional subsystems as communication ports 303 (including modem ports and modems, network interfaces, and the like), graphics display system 304 (including video memory, video processors, a graphics monitor), main memory system 305 which is typically implemented by way of dynamic random access memory (DRAM) and which may include memory stack 307, input devices 306 (including keyboard, a pointing device, and the interface circuitry therefor), and disk system 308 (which may include hard disk drives, floppy disk drives, and CD-ROM drives). It is therefore contemplated that system 300 of FIG. 1 corresponds to a conventional desktop computer or workstation, as are now common in the art. Of course, other system implementations of microprocessor 10 can also benefit from the present invention, as will be recognized by those of ordinary skill in the art.</p><p>Microprocessor 10 includes bus interface unit (BIU) 12 connected to external bus BUS, which controls and effects communication between microprocessor 10 and the other elements in a system 300. BIU 12 includes the appropriate control and clock circuitry to perform this function, including write buffers for increasing the speed of operation, and including timing circuitry so as to synchronize the results of internal microprocessor operation with bus BUS timing constraints. Microprocessor 10 also includes clock generation and control circuitry 20 which generates clock phases based upon system clock SYSCLK; in this example, clock generation and control circuitry 20 generates bus clock BCLK and core clock PCLK from system clock SYSCLK.</p><p>As is evident in FIG. 1, microprocessor 10 has three levels of internal cache memory, with the highest of these as level 2 cache 11, which is connected to BIU 12 by way of an internal bus. In this example, level 2 cache 11 is a unified cache, and is configured to receive all cacheable data and cacheable instructions from bus BUS via BIU 12, such that much of the bus traffic presented by microprocessor 10 is accomplished via level 2 cache 11. Microprocessor 10 may also effect bus traffic around cache 11, by treating certain bus reads and writes as \"not cacheable\". Level 2 cache 11, as shown in FIG. 2, is connected to two level 1 caches 16; level 1 data cache 16<sub>d</sub> is dedicated to data, while level 1 instruction cache 16<sub>i</sub> is dedicated to instructions. Microcache 18 is a fully dual-ported level 0 data cache, in this example. Main translation look-aside buffer (TLB) 19 controls memory accesses to level 2 cache 11 and to main memory via BIU 12, such control including the sequencing of accesses to the page tables in memory for address translation. TLB 19 also serves as a cache for the page tables. Instruction micro-translation lookaside buffer (\u03bcTLB) 22 and data microtranslation lookaside buffer (\u03bcTLB) 38 are provided to translate logical data addresses into physical addresses for accesses to level 1 instruction cache 16<sub>i</sub> and level 1 data cache 16<sub>d</sub>, respectively, in the conventional manner.</p><p>As shown in FIG. 1, microprocessor 10 is of the superscalar type, and thus includes multiple execution units. These execution units include two ALUs 42<sub>0</sub>, 42<sub>1</sub> for processing conditional branch, integer, and logical operations, floating-point unit (FPU) 31, two load-store units 40<sub>0</sub>, 40<sub>1</sub>, and microsequencer 48. The two load-store units 40 utilize the two ports to microcache 18, for true parallel access thereto, and also perform load and store operations to registers in register file 39. As conventional in the art, register file 39 includes general purpose registers that are available for programmer use, and also control registers including code segment register CS.</p><p>These multiple execution units are controlled by way of multiple pipelines of seven stages each, with write-back. The pipeline stages are as follows:</p><p></p><pre listing-type=\"tabular\" xml:space=\"preserve\"><!--Greenbook tabular data-->______________________________________\nF    Fetch: This stage generates the instruction address and reads the\n     instruction from the instruction cache or memory\nPD0  Predecode stage 0: This stage determines the length and starting\n     position of up to three fetched x86-type instructions\nPD1  Predecode stage 1: This stage extracts the x86 instruction bytes\n     and recodes them into fixed length format for decode\nDC   Decode: This stage translates the x86 instructions into atomic\n     operations (AOps)\nSC   Schedule: This stage assigns up to four AOps to the appropriate\n     execution units (including FPU 31)\nOP   Operand: This stage retrieves the register operands indicated by\n     the AOps\nEX   Execute: This stage runs the execution units according to the\n     AOps and the retrieved operands\nWB   Write-back: This stage stores the results of the execution in\n     registers or in memory\n______________________________________\n</pre><p>This pipeline, referred to below as the \"integer pipeline\", operates in combination with the floating-point pipeline of FPU 31 according to the preferred embodiment of the present invention.</p><p>Referring back to FIG. 1, the pipeline stages noted above are performed by various functional blocks within microprocessor 10. Fetch unit 26 generates instruction addresses from the instruction pointer by way of instruction micro-translation lookaside buffer (\u03bcTLB) 22, for application to level 1 instruction cache 16<sub>i</sub>, including according to branch prediction techniques as will be described in further detail below; in addition, as will also be described in detail below, fetch unit 26 receives signals on line U/S from code segment register CS indicating the program type, or class, of the current instruction at fetch unit 26. Instruction cache 16<sub>i</sub> produces a stream of instruction data to fetch unit 26, which in turn provides the instruction code to predecode 0 stage 28 and predecode 1 stage 32 in the desired sequence. These two stages operate as separate pipeline stages, and together operate to locate up to three x86 instructions and apply the same to decoder 34. Predecode 0 stage 28 determines the size and position of as many as three variable-length x86 instructions, while predecode 1 stage 32 recodes the multibyte instructions into a fixed-length format to facilitate decoding. Decode unit 34, in this example, contains four instruction decoders, each capable of receiving a fixed length x86 instruction from predecode 1 stage 32 and producing from one to three atomic operations (AOps), which are substantially equivalent to RISC instructions. Scheduler 36 reads up to four AOps from the decode queue at the output of decode unit 34, and assigns these AOps to the appropriate execution units. Operand unit 44 receives an input from scheduler 36 and also from microcode ROM 46, via multiplexer 45, and fetches register operands for use in the execution of the instructions. In addition, according to this example, operand unit 44 also performs operand forwarding to send results to registers that are ready to be stored, and also performs address generation for AOps of the load and store type.</p><p>Microsequencer 48 and microcode ROM 46 control ALUs 42 and load/store units 40 in the execution of microcode entry AOps, which are generally the last AOps to execute in a cycle. In this example, microsequencer 48 sequences through microinstructions stored in microcode ROM 46, to effect control responsive to microcoded microinstructions such as complex or rarely-used x86 instructions, x86 instructions that modify segment or control registers, handling of exceptions and interrupts, and multicycle instructions (such as REP instructions, and instructions that PUSH and POP all registers).</p><p>Microprocessor 10 also includes circuitry 24 for controlling the operation of JTAG scan testing, and of certain built-in self-test (BIST) functions, ensuring the validity of the operation of microprocessor 10 upon completion of manufacturing, and upon resets and other events.</p><p>Referring now to FIG. 2, the construction and operation of fetch unit 26 according to the preferred embodiment of the invention will now be described. As noted above, fetch unit 26 performs the function of determining the address of the next instruction to be fetched for decode. As such, fetch unit 26 determines the sequence in which instructions are loaded into the pipelines of microprocessor 10, and in this embodiment of the invention thus controls the speculative execution of addresses, particularly by way of branch prediction.</p><p>The operation of fetch unit 26 is based upon a logical fetch address FA that is generated according to one of several ways, as selected by multiplexer 52. Fetch address FA may be generated merely from the contents of fetch pointer 50 in fetch unit 26, in the case where the next sequential address is to be fetched for decoding. As shown in FIG. 2, fetch pointer 50 is a register in fetch unit 26, having an output connected to one input of multiplexer 52 and also to incrementer 51. Incrementer 51 advances the value of the fetch address to the next logical instruction (in the case of a superscalar machine, the next