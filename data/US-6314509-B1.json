-line buffers. Specifically, two sequential instruction cache lines each having a power of two size (e.g., 32 bytes) are stored in the dual in-line buffers. An instruction having a non-power of two size (e.g., 5 bytes, 10 bytes, 15 bytes, or 20 bytes) is extracted and aligned from the dual in-line buffers in a single clock cycle of the microprocessor. More specifically, an instruction cache line is stored in an instruction cache unit, in which the stored instruction cache line is power of two size-aligned. The instruction cache line is fetched and stored in a first line of the dual in-line buffers. Power of two size instruction cache data (e.g., 32 bytes of instruction cache data) is extracted from the dual in-line buffers and then transmitted to an instruction aligner. A rotate and truncate (RAT) unit of the instruction aligner rotates and truncates the power of two size instruction data to provide an instruction having a non-power of two size, which is then transmitted to an instruction queue for buffering before execution. For example, this method can be used for a microprocessor that implements an instruction set architecture, which includes instructions having a non-power of two size.</p><p>Other aspects and advantages of the present invention will become apparent from the following detailed description and accompanying drawings.</p><?BRFSUM description=\"Brief Summary\" end=\"tail\"?><?brief-description-of-drawings description=\"Brief Description of Drawings\" end=\"lead\"?><h4>BRIEF DESCRIPTION OF THE DRAWINGS</h4><p>FIG. 1 is a block diagram of a microprocessor that includes an instruction fetch unit in accordance with one embodiment of the present invention.</p><p>FIG. 2 shows various formats of instructions having a non-power of two size.</p><p>FIG. 3 is a block diagram of an instruction queue and the instruction fetch unit of FIG. 1 shown in greater detail in accordance with one embodiment of the present invention.</p><p>FIG. 4 is a functional diagram of the instruction cache unit of FIG. 1 connected to the instruction fetch unit of FIG. 1 in accordance with one embodiment of the present invention.</p><p>FIG. 5 is a diagram of possible 5-byte instruction positions within a 32-byte wide cache memory.</p><p>FIG. 6 is a functional diagram of the operation of the instruction fetch unit of FIG. 4 shown in greater detail in accordance with one embodiment of the present invention.</p><p>FIG. 7 is a functional diagram of a multi-level implementation of the instruction aligner of FIG. 3 in accordance with one embodiment of the present invention.</p><p>FIG. 8 is a block diagram of the line buffers connected to the double word muxes of the instruction fetch unit of FIG. 6 shown in greater detail in accordance with one embodiment of the present invention.</p><p>FIG. 9 is functional diagram of the operation of the rotate and truncate (RAT) unit of FIG. 6 shown in greater detail in accordance with one embodiment of the present invention.</p><p>FIG. 10 is a functional diagram of a symbolic implementation of the RAT unit of FIG. 6 in accordance with one embodiment of the present invention.</p><p>FIG. 11 is a functional diagram of a RAT bit ordering in accordance with one embodiment of the present invention.</p><p>FIG. 12 is a functional diagram of a RAT physical implementation in accordance with one embodiment of the present invention.</p><p>FIG. 13 is a functional diagram of an input byte ordering of each four byte group that allows the mux's select control signals to be shared in accordance with one embodiment of the present invention.</p><p>FIG. 14 is a block diagram of the instruction queue of FIG. 3 shown in greater detail in accordance with one embodiment of the present invention.</p><?brief-description-of-drawings description=\"Brief Description of Drawings\" end=\"tail\"?><?DETDESC description=\"Detailed Description\" end=\"lead\"?><h4>DETAILED DESCRIPTION</h4><p>A typical instruction set architecture (ISA) for a microprocessor specifies instructions has a power of two size, which can be aligned on a power of two boundary in a conventional cache memory. A typical ISA includes 32-bit instructions that are a fixed size such as for RISC (Reduced Instruction Set Computer) processors. The 32-bit instructions are typically aligned on a 32-bit boundary in a conventional instruction cache unit. The 32-bit instructions can be prefetched from the instruction cache unit in one clock cycle using a conventional 32-bit data path between the prefetch unit and the instruction cache unit.</p><p>However, new instruction set architectures may include instructions having a non-power of two size. To efficiently fetch instructions having a non-power of two size, a method in accordance with one embodiment of the present invention includes fetching at least two sequential cache lines for storage in line buffers of an instruction fetch unit of a microprocessor, and then efficiently extracting and aligning all the bytes of a non-power of two size instruction from the line buffers. This approach allows for a standard instruction cache architecture, which aligns cache lines on a power of two boundary, to be used. This approach also reduces the data path between the instruction cache and the instruction fetch unit. This approach sustains a fetch of always at least one sequential instruction per clock cycle of the microprocessor.</p><p>For example, an ISA can require supporting execution of instruction packets such as VLIW (Very Long Instruction Word) packets that are either 5, 10, 15, or 20 bytes wide. For certain applications such as graphics or media code, there may predominantly be 20-byte wide VLIW packets. If a 20-byte VLIW packet is executed per clock cycle (e.g., at a peak execution rate), then to maintain this peak execution rate, the instruction fetch unit fetches at least 20 bytes per clock cycle from the instruction cache unit.</p><p>FIG. 1 is a block diagram of a microprocessor <b>100</b> that includes an instruction fetch unit (IFU) <b>108</b> in accordance with one embodiment of the present invention. In particular, microprocessor <b>100</b> includes a main memory <b>102</b> connected to a bus <b>104</b>, an instruction cache unit <b>106</b> connected to bus <b>104</b>, instruction fetch unit <b>108</b> connected to instruction cache unit <b>106</b>, and P1 processor <b>110</b> and P2 processor <b>112</b> each connected to instruction fetch unit <b>108</b>. In one embodiment, P1 processor <b>110</b> is provided (i.e., instead of P1 processor <b>110</b> and P2 processor <b>112</b>), and P1 processor <b>110</b> is connected to instruction fetch unit <b>108</b>.</p><p>In one embodiment, instruction cache unit <b>106</b> is a conventional 16-kilobyte dual-ported cache that uses a well-known (standard) cache architecture of two-way set associative, 32-byte lines (e.g., in order to minimize cost and timing risk). Instruction cache unit <b>106</b> returns a new 32-byte cache line to instruction fetch unit <b>108</b> during each clock cycle of microprocessor <b>100</b>, and thus, instruction cache unit <b>106</b> can satisfy an execution rate of, for example, a 20-byte VLIW packet per clock cycle of microprocessor <b>100</b>.</p><p>However, the 20-byte VLIW packets may not be aligned on the 32-byte cache line boundaries of instruction cache unit <b>106</b>. VLIW packets can start on any byte boundary, and an empirical observation reveals that a significant number of the VLIW packets often start on a first cache line and continue onto a second cache line of two sequential cache lines. For VLIW packets that span two cache lines, two clock cycles would typically be needed to fetch the entire VLIW packet before executing the VLIW packet. As a result, the execution pipeline of microprocessor <b>100</b> may be reduced to approximately one half, thus resulting in a significant performance degradation.</p><p>Accordingly, instruction fetch unit <b>108</b> stores two instruction cache lines fetched from instruction cache unit <b>106</b> to ensure that instruction fetch unit <b>108</b> can provide the next VLIW packet, regardless of whether or not the VLIW packet spans two cache lines, in a single clock cycle. In particular, instruction fetch unit <b>108</b> prefetches ahead of execution, predicts branch outcomes, and maintains two sequential cache lines of unexecuted instructions. For example, a 20-byte VLIW packet is extracted from the two sequential instruction cache lines of instruction fetch unit <b>108</b> and then appropriately aligned, and the extraction and alignment is completed in one clock cycle (assuming the two sequential cache lines stored in instruction fetch unit <b>108</b> represent valid data). For sequential execution, instruction fetch unit <b>108</b> provides at least one VLIW packet per clock cycle, regardless of whether or not the VLIW packet spans two cache lines in instruction cache unit <b>106</b>.</p><p>In one embodiment, instruction cache unit <b>106</b> is a shared instruction cache unit for multiple processors (e.g., PI processor <b>110</b> and P2 processor <b>112</b>). A shared instruction cache for multiple processors is disclosed in commonly assigned and co-pending U.S. patent application Ser. No. 09/204,793 and incorporated herein by reference in its entirety.</p><p>A typical instruction fetch unit provides a 4-byte granularity. In contrast, instruction fetch unit <b>108</b> provides a 1-byte granularity and can fetch instructions with a 1-byte granularity. Instruction fetch unit <b>108</b> extracts and aligns a 5, 10, 15, or 20 byte VLIW packet from 64 bytes of instruction data stored in instruction fetch unit <b>108</b> (e.g., an instruction cache line of an instruction cache unit <b>106</b> is 32-bytes). Instruction fetch unit <b>108</b> efficiently performs the align operation as discussed below.</p><p>FIG. 2 shows various formats of instructions having a non-power of two size. In particular, instruction format <b>202</b> shows an instruction format for a variable size opcode which includes an 8-bit to 16-bit opcode, a 6-bit to 10-bit destination, a 6-bit to 10-bit source <b>1</b>, a 6-bit to 10-bit source <b>2</b>, and a 6-bit to 10-bit source <b>3</b>. Format <b>202</b> ranges from 32 bits to 56 bits. Instruction format <b>204</b> shows a 40-bit instruction format which includes an 8-bit opcode, an 8-bit destination, an 8 bit source <b>1</b>, an 8-bit source <b>2</b> and an 8-bit source <b>3</b>.</p><p>Storing non-power of two size instructions, such as shown in instruction format <b>204</b>, in a conventional DRAM (Dynamic Random Access Memory) or other conventional cache memory that includes cache lines of power of two size (e.g., because of binary addressing) results in non-aligned instructions being stored in the instruction cache. Thus, one embodiment of the present invention allows for the fetching of non-power of two size instructions from an instruction cache unit in one clock cycle of the microprocessor. For example, a typical DRAM has a width of a power of two number of bits (e.g., 32 bytes). Similarly, on-chip memory is typically organized using power of two boundaries and addressing. Thus, non-power of two instruction sets, such as shown in the instruction format <b>204</b> (i.e., a forty bit or five byte instruction), are not necessarily aligned when stored in instruction cache unit <b>106</b>.</p><p>FIG. 3 is a block diagram of an instruction queue <b>302</b> and instruction fetch unit <b>108</b> shown in greater detail in accordance with one embodiment of the present invention. Instruction fetch unit <b>108</b> is connected to instruction cache unit <b>106</b> via a conventional 32-byte data path. Instruction fetch unit <b>108</b> includes a prefetch unit <b>304</b>. Prefetch unit <b>304</b> includes dual in-line buffers <b>306</b>. Dual in-line buffers <b>306</b> are implemented as, for example, two 32-byte wide registers. Dual in-line buffers <b>306</b> store two sequential lines of instructions fetched from instruction cache unit <b>106</b>. By storing two sequential lines of instructions fetched from instruction cache unit <b>106</b>, instruction fetch unit <b>108</b> essentially ensures that the subsequent instruction is stored in dual in-line buffers <b>306</b>, regardless of whether or not it represents a non-aligned instruction (e.g., the instruction spans two lines in instruction cache unit <b>106</b>). Thus, instruction fetch unit <b>108</b> solves the problem of having to request two instruction fetches from instruction cache unit <b>106</b>, which typically causes a waste of at least one clock cycle of the microprocessor.</p><p>Instruction fetch unit <b>108</b> also includes an instruction aligner <b>308</b>. Instruction aligner <b>308</b> extracts and aligns the non-power of two size instruction from instruction data stored in dual in-line buffers <b>306</b>. For example, for a 40-bit instruction, instruction aligner <b>308</b> extracts the 40-bit instruction from the 64 bytes of data stored in dual in-line buffers <b>306</b>. Instruction aligner <b>308</b> then efficiently aligns the 40-bit instruction, as further discussed below.</p><p>In one embodiment, microprocessor <b>100</b> includes four processors or CPUs (Central Processing Units). Microprocessor <b>100</b> executes up to four instructions per cycle. Instruction fetch unit <b>108</b> provides up to four instructions per cycle to instruction queue <b>302</b> to maintain the peak execution rate of four instructions per cycle. For example, for a 40-bit instruction set, which defines 40-bit instruction sizes, instruction fetch unit <b>108</b> provides up to 160 bits per cycle in order to provide four instructions per cycle. Thus, instruction fetch unit <b>108</b> provides up to 20-bytes of instruction data (e.g., a 20-byte VLIW packet) to instruction queue <b>302</b> per cycle. Because dual in-line buffers <b>306</b> store 64 bytes of instruction data, instruction aligner <b>308</b> is responsible for extracting and appropriately aligning, for example, the 20 bytes of instruction data for the next cycle that is within the 64 bytes of instruction data stored in dual in-line buffers <b>306</b>. Accordingly, in one embodiment, an efficient method for fetching instructions having a non-power of two size is provided.</p><p>FIG. 4 is a functional diagram of instruction cache unit <b>106</b> connected to instruction fetch unit <b>108</b> in accordance with one embodiment of the present invention. A cache line <b>402</b> that includes 32 bytes of instruction data stored in instruction cache unit <b>106</b> is sent to instruction fetch unit <b>108</b> via a 32-byte data path <b>404</b>. Instruction fetch unit <b>108</b> includes dual in-line buffers <b>306</b>. Dual in-line buffers <b>306</b> include a line buffer <b>0</b> that is 32-bytes wide and a line buffer <b>1</b> that is 32-bytes wide. For example, line buffer <b>0</b> and line buffer <b>1</b> can be implemented as registers of instruction fetch unit <b>108</b>, or line buffer <b>0</b> and line buffer <b>1</b> of dual in-line buffers <b>306</b> can be implemented as two sets of enable-reset flip-flops, in which the flip-flops can be stacked (two in one bit slice). The 32-bytes of data are then extracted from dual in-line buffers <b>306</b> and transmitted via a 32-byte data path <b>406</b> to instruction aligner <b>308</b>. Instruction aligner <b>308</b> extracts and aligns the instruction (e.g., 10 bytes of instruction data) from the 32 bytes of instruction data and then transmits the extracted and aligned instruction for appropriate execution on processors <b>110</b> and <b>112</b> of microprocessor <b>100</b>.</p><p>Dual in-line buffers <b>306</b> maintain two sequential lines of instruction data fetched from instruction cache unit <b>106</b>. After the instruction data is extracted from dual in-line buffers <b>306</b>, instruction fetch unit <b>108</b> fetches the next sequential line of instruction data for storage in dual in-line buffers <b>306</b>. For example, based on the address of the fetched data (e.g., if the fifth address bit is zero, then the fetched data is loaded into line buffer <b>0</b>, else the fetched data is loaded into line buffer <b>1</b>), either line buffer <b>0</b> or line buffer <b>1</b> is purged, and the next sequential line of cache memory (e.g., cache line <b>402</b> of instruction cache unit <b>106</b>) is fetched and stored in the now purged line buffer <b>0</b> or line buffer <b>1</b>. In steady state mode, instruction fetch unit <b>108</b> maintains a rate of fetching of 32 bytes of instruction data per cycle. Because only up to 20 bytes of instruction data are consumed per cycle in the 20-byte VLIW packet example, and instruction data is stored in memory sequentially, instruction fetch unit <b>108</b> can generally satisfy the peak execution rate of microprocessor <b>100</b>, such as 20 bytes of instruction data or four instructions per multi-processor cycle of microprocessor <b>100</b>.</p><p>The instruction data path within instruction fetch unit <b>108</b> involves, for example, selecting a 20-byte wide byte-aligned field from 64 bytes of data stored in dual in-line buffers <b>306</b>. The 20-byte wide byte-aligned field is buffered (e.g., stored in instruction queue <b>302</b>) and then appropriately presented to the CPUs (e.g., 4 different processors). For a 20-byte VLIW packet, the data path size between instruction cache unit <b>106</b> and instruction fetch unit <b>108</b> can be 32 bytes, because the cache line size is 32 bytes.</p><p>However, extracting a 20-byte wide byte-aligned field from 64 bytes of non-aligned instruction data efficiently represents a challenging problem. Accordingly, instruction fetch unit <b>108</b> efficiently performs a rotate and truncate (RAT) of a 20-byte wide byte-aligned field from 64 bytes of non-aligned instruction data, in which, for example, 20 bytes is the maximum size of a VLIW packet, and 64 bytes of instruction data is prefetched from instruction cache unit <b>106</b> in accordance with one embodiment of the present invention, as further discussed below.</p><p>FIG. 5 is a diagram of possible 5-byte instruction positions within a 32-byte wide cache memory. Each 32-byte aligned location is called a cache memory line. An i